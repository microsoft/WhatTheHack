{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18002aa6-39a1-4e71-8749-324e2f615f31",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Challenge 04-A - Retrieval Augmented Generation (RAG) for Structured Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4ca1751",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will explore the practical application of RAG with a more manageable type of data i.e structured data such as relational data or text data stored in csv files. The main objective is to introduce a specific use case that demonstrates the utilization of Azure Cognitive Search to extract relevant documents and the power of ChatGPT to address relevant portions of the document, providing concise summaries based on user prompts. It aims to showcase how Azure OpenAI's ChatGPT capabilities can be adapted to suit your summarization needs, while also guiding you through the setup and evaluation of summarization results. This method can be customized to suit various summarization use cases and applied to diverse datasets.\n",
    "\n",
    "Your goals for this challenge are to read through this notebook, run each code block, observe the results, and then be able to answer the questions posed in the student guide."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f2d0025-3952-481b-9615-cfe5ee198f66",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Use Case\n",
    "\n",
    "This use case consists of three sections:\n",
    "- Document Search - The process of extracting relevant documents based on the query from a corpus of documents.\n",
    "- Document Zone Search - The process of finding the relevant part of the document extracted from document search.\n",
    "- Downstream AI tasks such as Question Answering (aka Text summarization) - Text summarization is the process of creating summaries from large volumes of data while maintaining significant informational elements and content value.\n",
    "This use case can be useful in helping subject matter experts in finding relevant information from large document corpus.\n",
    "\n",
    "**Example:** In the drug discovery process, scientists in pharmaceutical industry read a corpus of documents to find specific information related to concepts, experiment results etc. This use case enables them to ask questions from the document corpus and the solution will come back with the succinct answer. Consequently, expediting the drug discovery process.\n",
    " \n",
    "Benefits of the solution:\n",
    "1. Shortens reading time\n",
    "2. Improves the effectiveness of searching for information\n",
    "3. Removes bias from human summarization techniques\n",
    "4. Increases bandwidth for humans to focus on more in-depth analysis \n",
    "\n",
    "\n",
    "The need for document summarization be applied to any subject matter (legal, financial, journalist, medical, academic, etc) that requires long document summarization. The subject matter that this notebook is focusing on is journalistic - we will walk through news articles.   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85743c37-40f6-493f-9eaa-e9c4857ba8eb",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### CNN & Daily Mail Dataset\n",
    "For this walkthrough, we will be using the CNN/Daily Mail dataset. This is a common dataset used for text summarization and question answering tasks. Human generated abstractive summary bullets were generated from news stories on the CNN and Daily Mail websites.\n",
    "\n",
    "\n",
    "### Data Description\n",
    "The relevant schema for our work today consists of:\n",
    "\n",
    "- `id`: a string containing the heximal formatted SHA1 hash of the URL where the story was retrieved from\n",
    "- `article`: a string containing the body of the news article\n",
    "- `highlights`: a string containing the highlight of the article as written by the article author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Azure Cognitive Search, OpenAI, and other python modules\n",
    "\n",
    "import os, json, requests, sys, re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cc4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is secure and recommended way to load OpenAI resource credentials and deployment names\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model=os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c44e3",
   "metadata": {},
   "source": [
    "**NOTE:** The path in the code cell below is referring to the `cnn_dailymail.csv` file in the `/data/structured/` folder. You may need to update this path if you are running this notebook from a different location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CNN dailymail dataset in pandas dataframe\n",
    "df = pd.read_csv('../data/structured/cnn_dailymail_data.csv') #path to CNN daily mail dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eff67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Cognitive Search Index client\n",
    "service_endpoint = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\")   \n",
    "key = os.getenv(\"AZURE_COGNITIVE_SEARCH_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_name = \"news-index\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b30a67db",
   "metadata": {},
   "source": [
    "### Define Index Fields and Create a Semantic Configuration\n",
    "\n",
    "A *semantic configuration* specifies how fields are used in semantic ranking. It gives the underlying models hints about which index fields are most important for semantic ranking, captions, highlights, and answers.\n",
    "\n",
    "You can add or update a semantic configuration at any time without rebuilding your index. When you issue a query, you'll add the semantic configuration (one per query) that specifies which semantic configuration to use for the query.\n",
    "\n",
    "Review the properties you'll need to specify. A semantic configuration has a name and at least one each of the following properties:\n",
    "\n",
    "* Title field - A title field should be a concise description of the document, ideally a string that is under 25 words. This field could be the title of the document, name of the product, or item in your search index. If you don't have a title in your search index, leave this field blank.\n",
    "* Content fields - Content fields should contain text in natural language form. Common examples of content are the body of a document, the description of a product, or other free-form text.\n",
    "* Keyword fields - Keyword fields should be a list of keywords, such as the tags on a document, or a descriptive term, such as the category of an item.\n",
    "\n",
    "You can only specify one title field but you can specify as many content and keyword fields as you like. For content and keyword fields, list the fields in priority order because lower priority fields may get truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"highlights\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"article\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        #title_field=SemanticField(field_name=\"\"), # title field is not present in the dataset. We can use OpenAI to generate title\n",
    "        #prioritized_keywords_fields=[SemanticField(field_name=\"\")], # keywords are not present in the dataset. We can use OpenAI to generate keywords\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"article\"), SemanticField(field_name=\"highlights\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframe to a list of dictionaries\n",
    "documents = df.to_dict('records')\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded and Indexed {len(result)} documents\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc09a779-e3cd-485f-ae3a-297491d993b0",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 1: Leveraging Cognitive Search to extract relevant article based on the query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32689db7-4337-42d9-b8f9-4cbd9d98a850",
   "metadata": {
    "gather": {
     "logged": 1675138710195
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#Extracting relevant article based on the query. eg: Clinton Democratic Nomination\n",
    "results = search_client.search(search_text=\"Clinton Democratic nomination\", include_total_count=True)\n",
    "document = next(results)['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9681f2-2448-4e6d-8174-5fb5ff61d5db",
   "metadata": {
    "gather": {
     "logged": 1675139624461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02375dcd-514e-4203-951e-729b3de07570",
   "metadata": {
    "gather": {
     "logged": 1675139635796
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#length of article extracted from Azure Cognitive search\n",
    "len(document) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30b4b060-1dca-468c-a1f5-ac1b9e5d4878",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 2: Document Zone Search\n",
    "### Document Zone: Azure OpenAI Embedding API\n",
    "Now that we narrowed on a single document from our knowledge base using Azure Cognitive Search, we can dive deeper into the single document to refine our initial query to a more specific section or \"zone\" of the article.\n",
    "\n",
    "To do this, we will utilize the Azure Open AI Embeddings API.\n",
    "\n",
    "### **Embeddings Overview**\n",
    "An embedding is a special format of data representation that can be easily utilized by machine learning models and algorithms. The embedding is an information dense representation of the semantic meaning of a piece of text. Each embedding is a vector of floating-point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity between two inputs in the original format. For example, if two texts are similar, then their vector representations should also be similar.\n",
    "\n",
    "Different Azure OpenAI embedding models are specifically created to be good at a particular task. Similarity embeddings are good at capturing semantic similarity between two or more pieces of text. Text search embeddings help measure long documents are relevant to a short query. Code search embeddings are useful for embedding code snippets and embedding nature language search queries.\n",
    "\n",
    "Embeddings make it easier to do machine learning on large inputs representing words by capturing the semantic similarities in a vector space. Therefore, we can use embeddings to if two text chunks are semantically related or similar, and inherently provide a score to assess similarity.\n",
    "\n",
    "### **Cosine Similarity**\n",
    "A previously used approach to match similar documents was based on counting maximum number of common words between documents. This is flawed since as the document size increases, the overlap of common words increases even if the topics differ. Therefore cosine similarity is a better approach.\n",
    "\n",
    "Mathematically, cosine similarity measures the cosine of the angle between two vectors projected in a multi-dimensional space. This is beneficial because if two documents are far apart by Euclidean distance because of size, they could still have a smaller angle between them and therefore higher cosine similarity.\n",
    "\n",
    "The Azure OpenAI embeddings rely on cosine similarity to compute similarity between documents and a query."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cf78f21-368a-4314-ab59-f5be527e4b08",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Setting up Azure OpenAI service and using deployed models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c178e3be",
   "metadata": {},
   "source": [
    "### **Chunking**\n",
    "\n",
    "Let's start with chunking. Why is chunking important when working with LLMs?\n",
    "\n",
    "Chunking helps overcome the challenges associated with processing long sequences and ensures optimal performance when working with LLMs.\n",
    "\n",
    "**Mitigating Token Limitations:** LLMs have a maximum token limit for each input sequence. If a document or input exceeds this limit, it needs to be divided into chunks that fit within the token constraints. Chunking allows the LLM to handle long documents or inputs by splitting them into multiple chunks that fall within the token limit. This ensures that the model can effectively process the entire content while adhering to the token constraints.\n",
    "\n",
    "**Memory and Computational Efficiency:** LLMs are computationally expensive and require substantial memory resources to process long sequences of text. Chunking involves breaking down long documents or input into smaller, manageable chunks, allowing the LLM to process them efficiently within its memory limitations. By dividing the input into smaller parts, chunking helps avoid memory errors or performance degradation that may occur when processing lengthy sequences.\n",
    "\n",
    "**Contextual Coherence:** Chunking helps maintain contextual coherence in the generated outputs. Instead of treating the entire input as a single sequence, breaking it into smaller chunks allows the model to capture local context more effectively. This improves the model's understanding of the relationships and dependencies within the text, leading to more coherent and meaningful generated responses.\n",
    "\n",
    "**Improved Parallelism:** Chunking enables parallel processing, which is essential for optimizing the performance of LLMs. By dividing the input into chunks, multiple chunks can be processed simultaneously, taking advantage of parallel computing capabilities. This leads to faster inference times and enhances overall efficiency when working with LLMs.\n",
    "\n",
    "We will be leveraging a basic splitter for this notebook. However, it's important to note that there are more advanced splitters available, which may better suit your specific use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043fbb4",
   "metadata": {
    "gather": {
     "logged": 1675138711079
    }
   },
   "outputs": [],
   "source": [
    "#Defining helper functions\n",
    "#Splits text after sentences ending in a period. Combines n sentences per chunk.\n",
    "def splitter(n, s):\n",
    "    pieces = s.split(\". \")\n",
    "    list_out = [\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n)]\n",
    "    return list_out\n",
    "\n",
    "# Perform light data cleaning (removing redudant whitespace and cleaning up punctuation)\n",
    "def normalize_text(s, sep_token = \" \\n \"):\n",
    "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
    "    s = re.sub(r\". ,\",\"\",s)\n",
    "    # remove all instances of multiple spaces\n",
    "    s = s.replace(\"..\",\".\")\n",
    "    s = s.replace(\". .\",\".\")\n",
    "    s = s.replace(\"\\n\", \"\")\n",
    "    s = s.strip()    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56354758-427f-4af9-94b9-96a25946e9a5",
   "metadata": {
    "gather": {
     "logged": 1675138711316
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "document_chunks = splitter(10, normalize_text(document)) #splitting extracted document into chunks of 10 sentences\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3c83f-deca-493b-aa41-12b89f24feff",
   "metadata": {
    "gather": {
     "logged": 1675138711716
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Handling Rate Limits\n",
    "\n",
    "from openai.error import RateLimitError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def get_embedding(text: str, engine: str = \"text-embedding-ada-002\"):\n",
    "    count=0\n",
    "    while True:\n",
    "        try:\n",
    "            embedding = openai.Embedding().create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
    "            break;\n",
    "        except RateLimitError:\n",
    "            count+=1\n",
    "            #print(f'RateLimitError Count: {count}')\n",
    "            sleep(2)            \n",
    "    return np.array(embedding).astype(np.float32)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\", temperature=0): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de635ba5-7cf1-4d4f-8598-73619fc9c7ef",
   "metadata": {
    "gather": {
     "logged": 1675138711984
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "embed_df = pd.DataFrame(document_chunks, columns = [\"chunks\"]) #datframe with document chunks\n",
    "\n",
    "#Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "embed_df['embeddings'] = embed_df[\"chunks\"].apply(lambda x : get_embedding(x, engine = embedding_model))\n",
    "\n",
    "embed_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7adb8-93dd-4dfd-995a-8df893a98d99",
   "metadata": {
    "gather": {
     "logged": 1675138712417
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# search through the document for a text segment most similar to the query\n",
    "# display top two most similar chunks based on cosine similarity\n",
    "def search_docs(df, user_query, top_n=3):\n",
    "    embedding = get_embedding(\n",
    "        user_query,\n",
    "        engine=embedding_model,\n",
    "    )\n",
    "    df[\"similarities\"] = df['embeddings'].apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df.sort_values(\"similarities\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .head(top_n)\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8511f3a-198f-4e8f-8a5d-cb74456826fa",
   "metadata": {
    "gather": {
     "logged": 1675138712650
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "document_specific_query = \"trouble so far in clinton campaign\" \n",
    "res = search_docs(embed_df, document_specific_query, top_n=2) #finding top 2 results based on similarity \n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eabac33e-5a98-49f0-8fd6-2750bcf79bb1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 3: Text Summarization\n",
    "\n",
    "This section will cover the end-to-end flow of using the GPT-3 and ChatGPT models for summarization tasks. \n",
    "The model used by the Azure OpenAI service is a generative completion call which uses natural language instructions to identify the task being asked and skill required – aka Prompt Engineering. Using this approach, the first part of the prompt includes natural language instructions and/or examples of the specific task desired. The model then completes the task by predicting the most probable next text. This technique is known as \"in-context\" learning. \n",
    "\n",
    "There are three main approaches for in-context learning: Zero-shot, Few-shot and Fine tuning. These approaches vary based on the amount of task-specific data that is given to the model: \n",
    "\n",
    "**Zero-shot**: In this case, no examples are provided to the model and only the task request is provided. \n",
    "\n",
    "**Few-shot**: In this case, a user includes several examples in the call prompt that demonstrate the expected answer format and content. \n",
    "\n",
    "**Fine-Tuning**: Fine Tuning lets you tailor models to your personal datasets. This customization step will let you get more out of the service by providing: \n",
    "-\tWith lots of data (at least 500 and above) traditional optimization techniques are used with Back Propagation to re-adjust the weights of the model – this enables higher quality results than mere zero-shot or few-shot. \n",
    "-\tA customized model improves the few-shot learning approach by training the model weights on your specific prompts and structure. This lets you achieve better results on a wider number of tasks without needing to provide examples in the prompt. The result is less text sent and fewer tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Designing a prompt that will show and tell GPT-3 how to proceed. \n",
    "+ Providing an instruction to summarize the text about the general topic (prefix)\n",
    "+ Providing quality data for the chunks to summarize and specifically mentioning they are the text provided (context + context primer)\n",
    "+ Providing a space for GPT-3 to fill in the summary to follow the format (suffix)\n",
    "'''\n",
    "\n",
    "# result_1 corresponding to the top chunk from Section 2. result_2 corresponding to the second to top chunk from section 2. \n",
    "# change index for desired chunk\n",
    "result_1 = res.chunks[0]\n",
    "result_2 = res.chunks[1]\n",
    "prompt_i = 'Summarize the content about the Clinton campaign given the text provided.\\n\\nText:\\n'+\" \".join([normalize_text(result_1)])+ '\\n\\nText:\\n'+ \" \".join([normalize_text(result_2)])+'\\n\\nSummary:\\n'\n",
    "print(prompt_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec85c16-daec-4eb3-aa33-bed7c20774b6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "get_completion(prompt_i, model=chat_model) # default temperature is set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449c949-1a01-4f20-bebb-e7674ac6de43",
   "metadata": {
    "gather": {
     "logged": 1675138714150
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# bumping up the temperature to 0.5 to increase the randomness of the model's output\n",
    "get_completion(prompt_i, model=chat_model, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f8afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2139c70ac98f3202d028164a545621647e07f47fd6f5d8ac55cf952bf7c15ed1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
