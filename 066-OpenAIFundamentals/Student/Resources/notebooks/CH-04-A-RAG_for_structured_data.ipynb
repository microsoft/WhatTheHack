{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18002aa6-39a1-4e71-8749-324e2f615f31",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Challenge 04-A - Retrieval Augmented Generation (RAG) for Structured Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4ca1751",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will explore the practical application of RAG with a more manageable type of data i.e structured data such as relational data or text data stored in csv files. The main objective is to introduce a specific use case that demonstrates the utilization of Azure Cognitive Search to extract relevant documents and the power of ChatGPT to address relevant portions of the document, providing concise summaries based on user prompts. It aims to showcase how Azure OpenAI's ChatGPT capabilities can be adapted to suit your summarization needs, while also guiding you through the setup and evaluation of summarization results. This method can be customized to suit various summarization use cases and applied to diverse datasets.\n",
    "\n",
    "This notebook leverages **Semantic Kernel** as an orchestration framework to coordinate multiple AI services and manage the RAG workflow. Semantic Kernel provides:\n",
    "\n",
    "- **Service Management**: Centralized registration and access to Azure OpenAI services (chat completion and embeddings)\n",
    "- **Execution Orchestration**: Coordinated execution of embedding generation, similarity search, and response generation\n",
    "- **Configuration Management**: Unified handling of model parameters and execution settings\n",
    "- **Async Operations**: Efficient handling of concurrent AI service calls\n",
    "\n",
    "The kernel acts as the central hub that orchestrates the interaction between Azure Cognitive Search for document retrieval and Azure OpenAI for embeddings and completions.\n",
    "\n",
    "## Student Tasks\n",
    "Your goals for this challenge are to read through this notebook and complete the code where there is a TODO comment. Use Github Copilot to write the code! Ensure you run each code block, observe the results, and then be able to answer the questions posed in the student guide."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f2d0025-3952-481b-9615-cfe5ee198f66",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Use Case\n",
    "\n",
    "This use case consists of three sections:\n",
    "- Document Search - The process of extracting relevant documents based on the query from a corpus of documents.\n",
    "- Document Zone Search - The process of finding the relevant part of the document extracted from document search.\n",
    "- Downstream AI tasks such as Question Answering (aka Text summarization) - Text summarization is the process of creating summaries from large volumes of data while maintaining significant informational elements and content value.\n",
    "This use case can be useful in helping subject matter experts in finding relevant information from large document corpus.\n",
    "\n",
    "**Example:** In the drug discovery process, scientists in pharmaceutical industry read a corpus of documents to find specific information related to concepts, experiment results etc. This use case enables them to ask questions from the document corpus and the solution will come back with the succinct answer. Consequently, expediting the drug discovery process.\n",
    " \n",
    "Benefits of the solution:\n",
    "1. Shortens reading time\n",
    "2. Improves the effectiveness of searching for information\n",
    "3. Removes bias from human summarization techniques\n",
    "4. Increases bandwidth for humans to focus on more in-depth analysis \n",
    "\n",
    "\n",
    "The need for document summarization be applied to any subject matter (legal, financial, journalist, medical, academic, etc) that requires long document summarization. The subject matter that this notebook is focusing on is journalistic - we will walk through news articles.   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85743c37-40f6-493f-9eaa-e9c4857ba8eb",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### CNN & Daily Mail Dataset\n",
    "For this walkthrough, we will be using the CNN/Daily Mail dataset. This is a common dataset used for text summarization and question answering tasks. Human generated abstractive summary bullets were generated from news stories on the CNN and Daily Mail websites.\n",
    "\n",
    "\n",
    "### Data Description\n",
    "The relevant schema for our work today consists of:\n",
    "\n",
    "- `id`: a string containing the heximal formatted SHA1 hash of the URL where the story was retrieved from\n",
    "- `article`: a string containing the body of the news article\n",
    "- `highlights`: a string containing the highlight of the article as written by the article author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Azure Cognitive Search, Semantic Kernel, and other python modules\n",
    "\n",
    "import os, json, requests, sys, re\n",
    "import asyncio\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Azure Cognitive Search imports\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "# Semantic Kernel imports for core services\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, AzureTextEmbedding\n",
    "from semantic_kernel.contents import ChatHistory\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cc4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and initialize Semantic Kernel services\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model = os.environ['EMBEDDING_MODEL_NAME']\n",
    "\n",
    "# Initialize Semantic Kernel\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# Add Azure OpenAI Chat Completion service\n",
    "chat_service = AzureChatCompletion(\n",
    "    deployment_name=chat_model,\n",
    "    endpoint=os.environ['OPENAI_API_BASE'],\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "kernel.add_service(chat_service)\n",
    "\n",
    "# Add Azure OpenAI Text Embedding service  \n",
    "embedding_service = AzureTextEmbedding(\n",
    "    deployment_name=embedding_model,\n",
    "    endpoint=os.environ['OPENAI_API_BASE'],\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "kernel.add_service(embedding_service)\n",
    "\n",
    "print(\"Semantic Kernel services initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c44e3",
   "metadata": {},
   "source": [
    "**NOTE:** The path in the code cell below is referring to the `cnn_dailymail.csv` file in the `/data/structured/` folder. You may need to update this path if you are running this notebook from a different location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CNN dailymail dataset in pandas dataframe\n",
    "df = pd.read_csv('../data/structured/cnn_dailymail_data.csv') #path to CNN daily mail dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eff67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Cognitive Search Index client\n",
    "service_endpoint = os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")   \n",
    "key = os.getenv(\"AZURE_AI_SEARCH_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_name = \"news-index\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b30a67db",
   "metadata": {},
   "source": [
    "### Define Index Fields and Create a Semantic Configuration\n",
    "\n",
    "A *semantic configuration* specifies how fields are used in semantic ranking. It gives the underlying models hints about which index fields are most important for semantic ranking, captions, highlights, and answers.\n",
    "\n",
    "You can add or update a semantic configuration at any time without rebuilding your index. When you issue a query, you'll add the semantic configuration (one per query) that specifies which semantic configuration to use for the query.\n",
    "\n",
    "Review the properties you'll need to specify. A semantic configuration has a name and at least one each of the following properties:\n",
    "\n",
    "* Title field - A title field should be a concise description of the document, ideally a string that is under 25 words. This field could be the title of the document, name of the product, or item in your search index. If you don't have a title in your search index, leave this field blank.\n",
    "* Content fields - Content fields should contain text in natural language form. Common examples of content are the body of a document, the description of a product, or other free-form text.\n",
    "* Keyword fields - Keyword fields should be a list of keywords, such as the tags on a document, or a descriptive term, such as the category of an item.\n",
    "\n",
    "You can only specify one title field but you can specify as many content and keyword fields as you like. For content and keyword fields, list the fields in priority order because lower priority fields may get truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"highlights\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"article\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        #title_field=SemanticField(field_name=\"\"), # title field is not present in the dataset. We can use OpenAI to generate title\n",
    "        #prioritized_keywords_fields=[SemanticField(field_name=\"\")], # keywords are not present in the dataset. We can use OpenAI to generate keywords\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"article\"), SemanticField(field_name=\"highlights\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4869992",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df.to_dict('records')\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded and Indexed {len(result)} documents\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc09a779-e3cd-485f-ae3a-297491d993b0",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 1: Leveraging Cognitive Search to extract relevant article based on the query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32689db7-4337-42d9-b8f9-4cbd9d98a850",
   "metadata": {
    "gather": {
     "logged": 1675138710195
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Semantic Kernel helper functions\n",
    "# These functions demonstrate the orchestration pattern where the kernel\n",
    "# manages service discovery and execution, providing a clean abstraction\n",
    "# over the underlying Azure OpenAI services\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureTextEmbedding, AzureChatCompletion\n",
    "\n",
    "#Student Task: Complete the get_embedding function\n",
    "async def get_embedding(kernel, text):\n",
    "    # Get the embedding service by type\n",
    "    #TODO: use semantic kernel and the AzureTextEmbedding function to generate embeddings and return embeddings. Sepcifically the generate_embeddings function\n",
    "    return embeddings\n",
    "\n",
    "async def get_completion(kernel, prompt, temperature):\n",
    "    # Get the chat service by type\n",
    "    chat_service = kernel.get_service(type=AzureChatCompletion)\n",
    "    \n",
    "    # Create execution settings\n",
    "    settings = AzureChatPromptExecutionSettings(\n",
    "        temperature=temperature,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    # Create a proper ChatHistory object with the user prompt\n",
    "    chat_history = ChatHistory()\n",
    "    chat_history.add_user_message(prompt)\n",
    "    \n",
    "    # Generate completion\n",
    "    response = await chat_service.get_chat_message_content(\n",
    "        chat_history=chat_history,\n",
    "        settings=settings\n",
    "    )\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "def search_similar_chunks(query_embedding, chunks_df, top_k=3):\n",
    "    similarities = []\n",
    "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "    \n",
    "    for idx, embedding in enumerate(chunks_df['embedding']):\n",
    "        embedding_array = np.array(embedding).reshape(1, -1)\n",
    "        similarity = cosine_similarity(query_embedding, embedding_array)[0][0]\n",
    "        similarities.append((similarity, idx))\n",
    "    \n",
    "    similarities.sort(reverse=True)\n",
    "    results = []\n",
    "    for similarity, idx in similarities[:top_k]:\n",
    "        results.append({\n",
    "            'text': chunks_df.iloc[idx]['text'],\n",
    "            'score': similarity\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9681f2-2448-4e6d-8174-5fb5ff61d5db",
   "metadata": {
    "gather": {
     "logged": 1675139624461
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Search for document about Laurene Jobs and Hillary Clinton\n",
    "search_query = \"Laurene Jobs Hillary Clinton\"\n",
    "results = search_client.search(search_text=search_query, top=1)\n",
    "\n",
    "document = list(results)[0]['article']\n",
    "print(f\"Retrieved document for query: '{search_query}'\")\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02375dcd-514e-4203-951e-729b3de07570",
   "metadata": {
    "gather": {
     "logged": 1675139635796
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#length of article extracted from Azure Cognitive search\n",
    "len(document) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4b060-1dca-468c-a1f5-ac1b9e5d4878",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 2: Document Zone Search\n",
    "### Document Zone: Semantic Kernel Orchestrated Embeddings\n",
    "Now that we narrowed on a single document from our knowledge base using Azure Cognitive Search, we can dive deeper into the single document to refine our initial query to a more specific section or \"zone\" of the article.\n",
    "\n",
    "To do this, we will utilize Semantic Kernel's orchestrated Azure OpenAI Embeddings service.\n",
    "\n",
    "### **Embeddings Overview**\n",
    "An embedding is a special format of data representation that can be easily utilized by machine learning models and algorithms. The embedding is an information dense representation of the semantic meaning of a piece of text. Each embedding is a vector of floating-point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity between two inputs in the original format. For example, if two texts are similar, then their vector representations should also be similar.\n",
    "\n",
    "Different Azure OpenAI embedding models are specifically created to be good at a particular task. Similarity embeddings are good at capturing semantic similarity between two or more pieces of text. Text search embeddings are useful for evaluating the relevance of long documents to a brief query. Code search embeddings are useful for embedding code snippets and embedding natural language search queries.\n",
    "\n",
    "Embeddings make it easier to do machine learning on large inputs representing words by capturing the semantic similarities in a vector space. Therefore, we can use embeddings to see if two text chunks are semantically related or similar, and inherently provide a score to assess similarity.\n",
    "\n",
    "### **Cosine Similarity**\n",
    "A previously used approach to match similar documents was based on counting maximum number of common words between documents. This is flawed since as the document size increases, the overlap of common words increases even if the topics differ. Therefore cosine similarity is a better approach.\n",
    "\n",
    "Mathematically, cosine similarity measures the cosine of the angle between two vectors projected in a multi-dimensional space. This is beneficial because if two documents are far apart by Euclidean distance because of size, they could still have a smaller angle between them and therefore higher cosine similarity.\n",
    "\n",
    "The Azure OpenAI embeddings rely on cosine similarity to compute similarity between documents and a query."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c178e3be",
   "metadata": {},
   "source": [
    "### **Chunking**\n",
    "\n",
    "Let's start with chunking. Why is chunking important when working with LLMs?\n",
    "\n",
    "Chunking helps overcome the challenges associated with processing long sequences and ensures optimal performance when working with LLMs.\n",
    "\n",
    "**Mitigating Token Limitations:** LLMs have a maximum token limit for each input sequence. If a document or input exceeds this limit, it needs to be divided into chunks that fit within the token constraints. Chunking allows the LLM to handle long documents or inputs by splitting them into multiple chunks that fall within the token limit. This ensures that the model can effectively process the entire content while adhering to the token constraints.\n",
    "\n",
    "**Memory and Computational Efficiency:** LLMs are computationally expensive and require substantial memory resources to process long sequences of text. Chunking involves breaking down long documents or input into smaller, manageable chunks, allowing the LLM to process them efficiently within its memory limitations. By dividing the input into smaller parts, chunking helps avoid memory errors or performance degradation that may occur when processing lengthy sequences.\n",
    "\n",
    "**Contextual Coherence:** Chunking helps maintain contextual coherence in the generated outputs. Instead of treating the entire input as a single sequence, breaking it into smaller chunks allows the model to capture local context more effectively. This improves the model's understanding of the relationships and dependencies within the text, leading to more coherent and meaningful generated responses.\n",
    "\n",
    "**Improved Parallelism:** Chunking enables parallel processing, which is essential for optimizing the performance of LLMs. By dividing the input into chunks, multiple chunks can be processed simultaneously, taking advantage of parallel computing capabilities. This leads to faster inference times and enhances overall efficiency when working with LLMs.\n",
    "\n",
    "We will be leveraging a basic splitter for this notebook. However, it's important to note that there are more advanced splitters available, which may better suit your specific use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea256fa6",
   "metadata": {},
   "source": [
    "### Orchestrated Multi-Step Processing\n",
    "\n",
    "The following section demonstrates how Semantic Kernel orchestrates a multi-step RAG process:\n",
    "1. **Document Chunking**: Breaking down retrieved documents into manageable pieces\n",
    "2. **Embedding Generation**: Using the orchestrated embedding service to create vector representations\n",
    "3. **Similarity Search**: Coordinating the search across embedded chunks\n",
    "4. **Response Generation**: Using the orchestrated chat service to generate final answers\n",
    "\n",
    "### Orchestration Benefits\n",
    "The Semantic Kernel orchestration pattern demonstrated in the helper functions above provides:\n",
    "- **Service Discovery**: Automatic lookup of registered services by type\n",
    "- **Lifecycle Management**: Proper initialization and cleanup of AI services  \n",
    "- **Configuration Consistency**: Centralized management of model parameters\n",
    "- **Error Handling**: Unified exception handling across all AI operations\n",
    "- **Async Coordination**: Efficient orchestration of concurrent AI service calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043fbb4",
   "metadata": {
    "gather": {
     "logged": 1675138711079
    }
   },
   "outputs": [],
   "source": [
    "# Text processing functions\n",
    "import re\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.replace(\"..\", \".\").replace(\". .\", \".\").replace(\"\\n\", \"\")\n",
    "    return text\n",
    "\n",
    "def split_into_chunks(text, sentences_per_chunk=5):\n",
    "    sentences = text.split(\". \")\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = \". \".join(sentences[i:i+sentences_per_chunk])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Create document chunks\n",
    "if document:\n",
    "    #TODO: add code to normalize text in document and split normalized document to chunks and\n",
    "    # store it in variable called document_chunks\n",
    "\n",
    "    print(f\"Created {len(document_chunks)} chunks from document\")\n",
    "else:\n",
    "    document_chunks = [\"Sample text chunk for testing\"]\n",
    "    print(\"Using sample chunks - no document found\")\n",
    "\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56354758-427f-4af9-94b9-96a25946e9a5",
   "metadata": {
    "gather": {
     "logged": 1675138711316
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create embeddings for document chunks\n",
    "embeddings = []\n",
    "for chunk in document_chunks:\n",
    "    #TODO: Generate embedding using get_embedding function and append to embeddings\n",
    "    \n",
    "\n",
    "# Create DataFrame with text and embeddings\n",
    "import pandas as pd\n",
    "chunks_df = pd.DataFrame({\n",
    "    'text': document_chunks,\n",
    "    'embedding': embeddings\n",
    "})\n",
    "\n",
    "print(f\"Generated embeddings for {len(chunks_df)} chunks\")\n",
    "\n",
    "# Demo search\n",
    "user_query = \"What did Laurene Jobs say about Hillary Clinton?\"\n",
    "query_embedding = await get_embedding(kernel, user_query)\n",
    "search_results = search_similar_chunks(query_embedding, chunks_df, top_k=3)\n",
    "\n",
    "print(f\"\\nQuery: {user_query}\")\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"\\nResult {i} (Score: {result['score']:.3f}):\")\n",
    "    print(result['text'][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3c83f-deca-493b-aa41-12b89f24feff",
   "metadata": {
    "gather": {
     "logged": 1675138711716
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Generate RAG response\n",
    "context = \"\\n\\n\".join([result['text'] for result in search_results])\n",
    "\n",
    "prompt = f\"\"\"Based on the following context, answer the question: {user_query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "response = await get_completion(kernel, prompt, temperature=0)\n",
    "print(\"RAG Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de635ba5-7cf1-4d4f-8598-73619fc9c7ef",
   "metadata": {
    "gather": {
     "logged": 1675138711984
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Alternative embedding DataFrame for compatibility\n",
    "embed_df = pd.DataFrame({\n",
    "    'chunks': document_chunks,\n",
    "    'embeddings': embeddings\n",
    "})\n",
    "\n",
    "embed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7adb8-93dd-4dfd-995a-8df893a98d99",
   "metadata": {
    "gather": {
     "logged": 1675138712417
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Document search function\n",
    "def search_docs(df, user_query, top_n=3):\n",
    "    query_embedding = asyncio.get_event_loop().run_until_complete(get_embedding(kernel, user_query))\n",
    "    query_embedding = np.array(query_embedding).reshape(1, -1)\n",
    "    \n",
    "    similarities = []\n",
    "    for idx, embedding in enumerate(df['embeddings']):\n",
    "        embedding_array = np.array(embedding).reshape(1, -1)\n",
    "        similarity = cosine_similarity(query_embedding, embedding_array)[0][0]\n",
    "        similarities.append((similarity, idx))\n",
    "    \n",
    "    similarities.sort(reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for similarity, idx in similarities[:top_n]:\n",
    "        results.append({\n",
    "            'chunks': df.iloc[idx]['chunks'],\n",
    "            'similarities': similarity\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Search for specific content\n",
    "query = \"trouble so far in clinton campaign\"\n",
    "results = search_docs(embed_df, query, top_n=2)\n",
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eabac33e-5a98-49f0-8fd6-2750bcf79bb1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Section 3: Text Summarization\n",
    "\n",
    "This section will cover the end-to-end flow of using the GPT-3 and ChatGPT models for summarization tasks. \n",
    "The model used by the Azure OpenAI service is a generative completion call which uses natural language instructions to identify the task being asked and skill required – aka Prompt Engineering. Using this approach, the first part of the prompt includes natural language instructions and/or examples of the specific task desired. The model then completes the task by predicting the most probable next text. This technique is known as \"in-context\" learning. \n",
    "\n",
    "There are three main approaches for in-context learning: Zero-shot, Few-shot and Fine tuning. These approaches vary based on the amount of task-specific data that is given to the model: \n",
    "\n",
    "**Zero-shot**: In this case, no examples are provided to the model and only the task request is provided. \n",
    "\n",
    "**Few-shot**: In this case, a user includes several examples in the call prompt that demonstrate the expected answer format and content. \n",
    "\n",
    "**Fine-Tuning**: Fine Tuning lets you tailor models to your personal datasets. This customization step will let you get more out of the service by providing: \n",
    "-\tWith lots of data (at least 500 and above) traditional optimization techniques are used with Back Propagation to re-adjust the weights of the model – this enables higher quality results than mere zero-shot or few-shot. \n",
    "-\tA customized model improves the few-shot learning approach by training the model weights on your specific prompts and structure. This lets you achieve better results on a wider number of tasks without needing to provide examples in the prompt. The result is less text sent and fewer tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summarization prompt\n",
    "result_1 = results.iloc[0]['chunks']\n",
    "result_2 = results.iloc[1]['chunks']\n",
    "print(f\"Selected chunks for summarization:\\n1. {result_1}\\n2. {result_2}\")\n",
    "\n",
    "prompt = f\"\"\"Summarize the content about the Clinton campaign from the following text:\n",
    "\n",
    "Text 1: {normalize_text(result_1)}\n",
    "\n",
    "Text 2: {normalize_text(result_2)}\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary = await get_completion(kernel, prompt, temperature=0.5)\n",
    "print(f\"Summary (Temperature=0.5):\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAG demonstration: Search + Summarize\n",
    "\n",
    "#TODO: Change this test query and see how it affects the results\n",
    "test_query = \"What are the key points about Clinton's campaign events?\"\n",
    "\n",
    "# Search for relevant documents\n",
    "search_results_df = search_docs(embed_df, test_query, top_n=3)\n",
    "\n",
    "# Create context from search results\n",
    "context = \"\\n\\n\".join(search_results_df['chunks'].tolist())\n",
    "\n",
    "# Generate RAG response using the context\n",
    "rag_prompt = f\"\"\"Based on the following context, answer the question: {test_query}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "final_response = await get_completion(kernel, rag_prompt, temperature=0.2)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nRAG Answer: {final_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
