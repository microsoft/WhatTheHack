{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e3964eae",
      "metadata": {},
      "source": [
        "# Challenge 5: Responsible AI"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "132f6f55",
      "metadata": {},
      "source": [
        "As LLMs grow in popularity and use around the world, the need to manage and monitor their outputs becomes increasingly important. In this challenge, you will learn how to evaluate the outputs of LLMs and how to identify and mitigate potential biases in the model.\n",
        "\n",
        "Questions you should be able to answer by the end of this challenge:\n",
        "- How can you leverage content filtering? \n",
        "- What are ways to evaluate truthfulness and reduce hallucinations?\n",
        "- How can you identify and mitigate bias in your model?\n",
        "\n",
        "Sections in this Challenge:\n",
        "\n",
        "1. Identifying harms and detecting Personal Identifiable Information (PII)<!--(#content-filtering,-content-safety,-and-personal-identifiable-information-(pii)-detection)-->\n",
        "1. Evaluating truthfulness using Ground Truth Datasets<!--(#evaluating-truthfulness-using-ground-truth-data)-->\n",
        "1. Evaluating truthfulness using GPT without Ground Truth Datasets<!--(#evaluating-truthfulness-using-gpt-without-ground-truth-datasets)-->\n",
        "\n",
        "Resources:\n",
        "- [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1fdf2ed6",
      "metadata": {},
      "source": [
        "## 1. Content filtering, Content Safety, and Personal Identifiable Information (PII) detection"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1e860826",
      "metadata": {},
      "source": [
        "The four stages of the Responsible AI recommendations when using OpenAI are to identify, measure, mitigate, and operate harms. In this section, we will focus on identifying harms.\n",
        "\n",
        "This step has the goal of identifying potential harms so you can effectively mitigate them. It's important to remember that identifying harms is highly dependent on the context. For example, a model that is used to generate text for a children's book will have different harms than a model that is used to generate text for a news article. Language will also have different meaning in different contexts, so an identification framework should be flexible enough to adapt to various situations.\n",
        "\n",
        "We present three tools to identifying harms:\n",
        "- Azure Cognitive Services Content Filtering\n",
        "- Azure AI Content safety\n",
        "- PII detection via OpenAI Plug-ins"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b5a13fe6",
      "metadata": {},
      "source": [
        "### 1.1 Azure Cognitive Services Content Filtering\n",
        "\n",
        "From [Azure documentation](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/content-filter): \n",
        "\n",
        "    Azure OpenAI Service includes a content management system that works alongside core models to filter content. This system works by running both the input prompt and generated content through an ensemble of classification models aimed at detecting misuse. \n",
        "\n",
        "You should evaluate all potential harms carefully and add scenario-specific mitigation as needed. For example, you may want to filter out content that is offensive, profane, sexually explicit, or hateful.\n",
        "\n",
        "**Knowledge Check #1**:\n",
        "\n",
        "To assess your understanding of the concept of content filtering, answer the following questions based on the documentation:\n",
        "\n",
        "* True or False: If you make a streaming completions request for multiple responses, the content filter will evaluate each response individually and return only the ones that pass.\n",
        "* True or False: the `finish_reason` parameter will be returned on every response from the content filter.\n",
        "* True or False: If the content filtering system is down, you will not be able to receive results about your request."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "67c23d99",
      "metadata": {},
      "source": [
        "### 1.2 Azure AI Content Safety (Preview)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c9e0d7a6",
      "metadata": {},
      "source": [
        "The [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/cognitive-services/content-safety/overview) was created to help organizations responsible manage and moderate user- and AI-generated content. It is a managed service that provides a scalable, low-latency, and cost-effective content moderation solution for your image and text content. It is designed to help you detect potentially unsafe content, including hate speech, violence, sexually explicit material, and self-harm.\n",
        "\n",
        "You can read more about the service in this [Microsoft article](https://techcommunity.microsoft.com/t5/ai-cognitive-services-blog/introducing-azure-ai-content-safety-helping-organizations-to/ba-p/3825744).\n",
        "\n",
        "**Knowledge Check #2**:\n",
        "\n",
        "Check your understanding of the AI Content Safety Service by answering the following questions:\n",
        "\n",
        "* True or False: The Text Moderation API is designed to support over 100 languages as input.\n",
        "* True or False: The AI Content Safety Service has a feature to monitor activity statistics of your application.\n",
        "* True or False: The Azure AI Content Safety Studio and the API have different risk scores (severity levels) across the categories of harm.\n",
        "* True or False: You can only customize severity thresholds through the API.\n",
        "* True or False: The API always returns a severity level for all four content categories."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "99f9e49d",
      "metadata": {},
      "source": [
        "To run the example, first install some packages and load your environment variables from a `.env` file.\n",
        "\n",
        "**NOTE:** The openai-python library support for Azure OpenAI is in preview. We have specified the API Preview version below.\n",
        "\n",
        "`os.getenv()` for the endpoint and key assumes that you are using environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220b62a1",
      "metadata": {
        "gather": {
          "logged": 1694716972271
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "assert API_KEY, \"ERROR: Azure OpenAI Key is missing\"\n",
        "openai.api_key = API_KEY\n",
        "RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_BASE\",\"\").strip()\n",
        "CHAT_MODEL = os.getenv(\"CHAT_MODEL_NAME\")\n",
        "assert RESOURCE_ENDPOINT, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
        "assert \"openai.azure.com\" in RESOURCE_ENDPOINT.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
        "openai.api_base = RESOURCE_ENDPOINT\n",
        "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
        "CHAT_INSTRUCT_MODEL = os.getenv(\"CHAT_INSTRUCT_MODEL\")\n",
        "openai.api_version = \"2023-06-01-preview\" # API version required to test out Annotations preview"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4fac5b67",
      "metadata": {},
      "source": [
        "Below is an example OpenAI call using the Preview version which enables [Annotations](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/content-filter#annotations-preview). Replace the input prompt with different text to see how the annotations change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee9a8bad",
      "metadata": {
        "gather": {
          "logged": 1694716864019
        }
      },
      "outputs": [],
      "source": [
        "pii_prompt = \"{Example prompt where a severity level of low is detected}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55521442",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = openai.Completion.create(\n",
        "    engine=CHAT_MODEL,\n",
        "    prompt=pii_prompt \n",
        "    # Content that is detected at severity level medium or high is filtered, \n",
        "    # while content detected at severity level low isn't filtered by the content filters.\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8f314ff6",
      "metadata": {},
      "source": [
        "### 1.3 Checking for PII data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "44894b31",
      "metadata": {},
      "source": [
        "Plugins are chat extensions designed specifically for language models like ChatGPT, enabling them to access up-to-date information, run computations, or interact with third-party services in response to a user's request. They unlock a wide range of potential use cases and enhance the capabilities of language models.\n",
        "\n",
        "The below function, `screen_text_for_pii`, can be helpful if you want to avoid uploading sensitive or private documents to a database unintentionally.\n",
        "\n",
        "This feature is not foolproof and may not catch all instances of personally identifiable information. Use this feature with caution and verify its effectiveness for your specific use case. You can read more about the background of this function from OpenAI [here](https://github.com/openai/chatgpt-retrieval-plugin/tree/main#plugins).\n",
        "\n",
        "For other ways to ensure your data is secure when using OpenAI, check out ways to [configure the OpenAI service with managed identities](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/managed-identity).\n",
        "\n",
        "Read through the function `screen_text_for_pii` in the cell below to understand how it works. You can replace the input text with information relevant to your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae37954e",
      "metadata": {
        "gather": {
          "logged": 1694716864051
        }
      },
      "outputs": [],
      "source": [
        "def get_completion_from_messages(messages, model=CHAT_MODEL, temperature=0):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        engine=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "def screen_text_for_pii(text: str) -> bool:\n",
        "    # This prompt is just an example, change it to fit your use case\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"\n",
        "            You can only respond with the word \"True\" or \"False\", where your answer indicates whether the text in the user's message contains PII.\n",
        "            Do not explain your answer, and do not use punctuation.\n",
        "            Your task is to identify whether the text extracted from your company files\n",
        "            contains sensitive PII information that should not be shared with the broader company. Here are some things to look out for:\n",
        "            - An email address that identifies a specific person in either the local-part or the domain\n",
        "            - The postal address of a private residence (must include at least a street name)\n",
        "            - The postal address of a public place (must include either a street name or business name)\n",
        "            - Notes about hiring decisions with mentioned names of candidates. The user will send a document for you to analyze.\n",
        "            \"\"\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": text},\n",
        "    ]\n",
        "\n",
        "    completion = get_completion_from_messages(messages)\n",
        "    \n",
        "    if completion.startswith(\"True\"):\n",
        "        return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "073cfbcb-dad4-47ba-8384-30a5d7a215b2",
      "metadata": {
        "gather": {
          "logged": 1694716864099
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Optional: test out the screening for PII using input data\n",
        "text = \"INPUT YOUR TEXT HERE\"\n",
        "screen_text_for_pii(text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3cadcf88",
      "metadata": {},
      "source": [
        "## 2. Evaluating truthfulness using Ground Truth data\n",
        "\n",
        "In this section, we will focus on evaluating truthfulness in model outputs. Model hallucinations is a common enough problem in using LLMs that it is important to evaluate whether the model is generating responses based on data rather than making up information. The goal is to improve truthfulness in results to make your model more consistent and reliable for production.\n",
        "\n",
        "This section will focus on how to evaluate your model when you have access to [Ground Truth](https://en.wikipedia.org/wiki/Ground_truth) data. This will allow us to compare the model's output to the correct answer. In the next section, we will focus on how to evaluate your model when you do not have access to Ground Truth data.\n",
        "\n",
        "When we use Ground Truth data, we can deduce a numerical representation of how similar the predicted answer is to the correct one using various metrics. You will also have the opportunity to identify and implement additional metrics to evaluate the use case in this section.\n",
        "\n",
        "We will evaluate models using datasets from Hugging Face as well as Hugging Face's [Evaluate library](https://huggingface.co/docs/evaluate/index).\n",
        "\n",
        "We will also be utilizing LangChain, which has a package (QAEvalChain) for this specific purpose. [Read more](https://python.langchain.com/en/latest/use_cases/evaluation/question_answering.html) about how Evaluation is implemented by LangChain. You may have heard of LangChain and Semantic Kernel. LangChain is a third-party, open-source framework that you can use to develop applications that are powered by language models. LangChain makes the complexities of working and building with AI models easier by providing the pipeline orchestration framework and helper utilities to run powerful, multiple-model pipelines. It can also be integrated with Prompt Flow to scale prompt engineering workflows.\n",
        "\n",
        "By the end of this section, you can review which approach (Hugging Face's Evaluate or LangChain's QAEvalChain) is preferable for future use cases."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0e3ce977",
      "metadata": {},
      "source": [
        "### 2.1 Setup\n",
        "\n",
        "For demonstration purposes, we will evaluate a simple question answering system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c10054f",
      "metadata": {
        "gather": {
          "logged": 1694716864133
        }
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import AzureChatOpenAI"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d4cfc9b3",
      "metadata": {},
      "source": [
        "Now we'll create a Prompt Template that will allow us to use the same prompt with different inputs. We will utilize [LangChain](https://docs.langchain.com/docs/), an open-source framework for working with language models.\n",
        "\n",
        "Read more about LangChain Chains and how they work [here](https://docs.langchain.com/docs/components/chains/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9abdf160",
      "metadata": {
        "gather": {
          "logged": 1694716864167
        }
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(template=\"Question: {question}\\nAnswer:\", input_variables=[\"question\"])\n",
        "llm = AzureChatOpenAI(deployment_name=CHAT_MODEL, temperature=0.9)\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cbea2132",
      "metadata": {},
      "source": [
        "### 2.2 Loading data\n",
        "\n",
        "Now we load a dataset from Hugging Face, and then convert it to a list of dictionaries for easier usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2373cf1",
      "metadata": {
        "gather": {
          "logged": 1694716864214
        }
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"truthful_qa\", \"generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b91f88b3-0b95-4814-bcd0-36527b0b49db",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Let's work with the first five examples in the [Truthful QA dataset from Hugging Face](https://huggingface.co/datasets/truthful_qa). We are working with the \"Generation\" subsection of the dataset because we are applying this to a text-generating system, but notice how there is another subsection for evaluating the model's performance on multiple choice scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e591ee7f",
      "metadata": {
        "gather": {
          "logged": 1694716864248
        }
      },
      "outputs": [],
      "source": [
        "num_examples = 3\n",
        "examples = list(dataset['validation'])[:num_examples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bf71517",
      "metadata": {
        "gather": {
          "logged": 1694716864285
        }
      },
      "outputs": [],
      "source": [
        "examples[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b8c3c8ef",
      "metadata": {},
      "source": [
        "### 2.3 Predictions\n",
        "\n",
        "We can now make and inspect the predictions for these questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b2849c",
      "metadata": {
        "gather": {
          "logged": 1694716864314
        }
      },
      "outputs": [],
      "source": [
        "predictions = chain.apply(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e1d71c",
      "metadata": {
        "gather": {
          "logged": 1694716864346
        }
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "de420cf5",
      "metadata": {},
      "source": [
        "### 2.4 Evaluation\n",
        "We can see that if we tried to just do exact match on the answer answers they would not match what the language model answered. However, semantically the language model is correct in both cases. In order to account for this, we can use a language model itself to evaluate the answers.\n",
        "\n",
        "Because these answers are more complex than multiple choice, we can now evaluate their accuracy using a language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e87e11",
      "metadata": {
        "gather": {
          "logged": 1694716864379
        }
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation.qa import QAEvalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc2e624",
      "metadata": {
        "gather": {
          "logged": 1694716864406
        }
      },
      "outputs": [],
      "source": [
        "# Create an Evaluation Chain using LangChain's QAEValChain\n",
        "eval_chain = QAEvalChain.from_llm(llm)\n",
        "graded_outputs = eval_chain.evaluate(examples, predictions, question_key=\"question\", answer_key=\"best_answer\", prediction_key=\"text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10238f86",
      "metadata": {
        "gather": {
          "logged": 1694716864440
        }
      },
      "outputs": [],
      "source": [
        "graded_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d216ad-4ae9-49a9-940c-84fccac2a343",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now we're going to count the number of outputs that were graded as \"Correct\" or \"Incorrect\" based on the evaluation from the QAEvalChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e70271",
      "metadata": {
        "gather": {
          "logged": 1694715843342
        }
      },
      "outputs": [],
      "source": [
        "num_correct = sum([1 for x in graded_outputs if str(x['results']).upper().startswith('CORRECT')])\n",
        "num_incorrect = sum([1 for x in graded_outputs if str(x['results']).upper().startswith('INCORRECT')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386764e3",
      "metadata": {
        "gather": {
          "logged": 1694715843378
        }
      },
      "outputs": [],
      "source": [
        "print(num_correct, num_incorrect)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e6dc737a",
      "metadata": {},
      "source": [
        "### 2.5 Comparing to other evaluation metrics\n",
        "\n",
        "We can compare the evaluation results we get to other common evaluation metrics. To do this, let’s load some evaluation metrics from HuggingFace’s Evaluate package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd5f575e",
      "metadata": {
        "gather": {
          "logged": 1694715843415
        }
      },
      "outputs": [],
      "source": [
        "print(examples[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "207be70d",
      "metadata": {
        "gather": {
          "logged": 1694715843446
        }
      },
      "outputs": [],
      "source": [
        "# Some data munging to get the examples in the right format\n",
        "for i, eg in enumerate(examples):\n",
        "    eg['id'] = str(i)\n",
        "    eg['answers'] = {\"text\": eg['correct_answers'], \"answer_start\": [0]}\n",
        "    predictions[i]['id'] = str(i)\n",
        "    predictions[i]['prediction_text'] = predictions[i]['text']\n",
        "\n",
        "for p in predictions:\n",
        "    del p['text']\n",
        "\n",
        "# references need id, answers as list with text and answer_start\n",
        "new_examples = examples.copy()\n",
        "# print(new_examples)\n",
        "for eg in new_examples:\n",
        "    del eg ['question']\n",
        "    del eg['best_answer']\n",
        "    del eg['type']\n",
        "    del eg['correct_answers']\n",
        "    del eg['category']\n",
        "    del eg['incorrect_answers']\n",
        "    del eg['source']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d7ef7ec-d7f9-4626-b4a9-12d002737796",
      "metadata": {
        "gather": {
          "logged": 1694715843488
        }
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "squad_metric = load(\"squad\")\n",
        "results = squad_metric.compute(\n",
        "    references=new_examples,\n",
        "    predictions=predictions,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2ab1dc",
      "metadata": {
        "gather": {
          "logged": 1694715843520
        }
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4df686a3",
      "metadata": {},
      "source": [
        "#### (Optional) Student Task\n",
        "\n",
        "Now add two additional metrics to evaluate the model using the Hugging Face Evaluate library. One of those could be the BERT_score metric.\n",
        "\n",
        "Resources for reference:\n",
        "\n",
        "* [Hugging Face's Evaluate Library on GitHub](https://github.com/huggingface/evaluate) \n",
        "* [Evaluate Library Documentation](https://huggingface.co/docs/transformers/tasks/translation#evaluate) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2424f5f7",
      "metadata": {
        "gather": {
          "logged": 1694715843567
        }
      },
      "outputs": [],
      "source": [
        "### STUDENT TASK ###"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f88f1a76",
      "metadata": {},
      "source": [
        "## 3. Evaluating Models for Truthfulness using GPT without Ground Truth Datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "767bceab",
      "metadata": {},
      "source": [
        "You won't always have Ground Truth data available to assess your model. Luckily, GPT does a really good job at generating Ground Truth data from your original dataset.\n",
        "\n",
        "Research has shown that LLMs such as GPT-3 and ChatGPT are good at assessing text inconsistency. Based on these findings, the models can be used to evaluate sentences for truthfulness by prompting GPT. Let's assess the accuracy of GPT through a technique of GPT evaluating itself.\n",
        "\n",
        "In this section, we will evaluate the model you worked on in the previous challenge applied to the CNN Dailymail dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b7e21f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain, QAGenerationChain\n",
        "from langchain.requests import Requests\n",
        "from langchain.llms import AzureOpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5a19c79e",
      "metadata": {},
      "source": [
        "### 3.1. Create a Ground Truth Dataset on Custom Data\n",
        "Let's start by using GPT to create a dataset of question-answer pairs as our \"ground-truth\" data from the CNN Dailymail dataset from the previous challenge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9681f823",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the provided CNN file, the path of which may change based on folder structure\n",
        "CNN_FILE_PATH = \"../data/structured/cnn_dailymail_data.csv\"\n",
        "\n",
        "# Optional: limit to 11 samples for simple scope to avoid RateLimitErrors\n",
        "# You are welcome to change `num_samples` or delete it to run this example on\n",
        "# the entire dataset but doing so may take 1+ hour\n",
        "num_samples = 11\n",
        "df = pd.read_csv(CNN_FILE_PATH)[:num_samples]\n",
        "df.drop([4,9], axis=0, inplace=True)\n",
        "df = df.drop(columns=[\"highlights\"])\n",
        "pd.set_option('display.max_colwidth', None)  # Show all columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9ad597",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Take a look at the data\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b447a9b6-3f2d-4bf9-84a5-4f269c78b45a",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Time for some data scrubbing for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6599f2f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the column \"article\" to a list of dictionaries\n",
        "df_copy = df.copy().rename(columns={\"article\": \"text\"})\n",
        "df_copy = df_copy.drop(columns=[\"id\"])\n",
        "df_dict = df_copy.to_dict(\"records\")\n",
        "\n",
        "print(df_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf8a1f8-369f-4fb4-909c-600e1c5102d3",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We've gone ahead and generated a question-answer pair for each article. This will help us assess GPT's performance on how well it answers the test questions. The answers in each pairing are considered our ground truth data and the ideal answer.\n",
        "\n",
        "We created these pairs using Langchain's [QAGenerationChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.qa_generation.base.QAGenerationChain.html#). Check out the [source code](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/qa_generation) to see how the question-answer pairs are being generated through QAGenerationChain. The implementation may surprise you!\n",
        "\n",
        "In the process, we removed articles that triggered the OpenAI content filter. \n",
        "\n",
        "Below, we're going to load the provided question-answer dataset for later assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af36819b",
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = AzureOpenAI(deployment_name=CHAT_MODEL, temperature=0, max_tokens=1000)\n",
        "chain = QAGenerationChain.from_llm(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dc5ba51",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load cnn_qa_set.json\n",
        "cnn_qa_set_filepath = '../data/structured/cnn_qa_set.json'\n",
        "with open(cnn_qa_set_filepath, 'r') as file:\n",
        "    qa_set = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b597401-c969-454f-a7ee-6a91195239f7",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "qa_set[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd04f598",
      "metadata": {},
      "source": [
        "Now we have the question and Ground Truth answers. Let's test the GPT + Cognitive Search solution you implemented in the last challenge! We are going to compare the differences between `truth_answers` (provided answers) and `prompt_answers` (model performance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c06d9ab0",
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = [(set[\"question\"] for set in qa_set)]\n",
        "truth_answers = [(set[\"answers\"] for set in qa_set)]\n",
        "prompt_answers = list()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8c9127ee",
      "metadata": {},
      "source": [
        "### 3.2 Instantiate the Cognitive Search Index\n",
        "\n",
        "We're using the Index you created in the last challenge to retrieve documents that are relevant to any input user query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c97e90f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, requests, sys, re\n",
        "import requests\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents.indexes import SearchIndexClient \n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes.models import (\n",
        "    SearchIndex,\n",
        "    SearchField,\n",
        "    SearchFieldDataType,\n",
        "    SimpleField,\n",
        "    SearchableField,\n",
        "    SemanticConfiguration,\n",
        "    PrioritizedFields,\n",
        "    SemanticField,\n",
        "    SemanticSettings\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a38c0644",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an SDK client\n",
        "service_endpoint = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\")   \n",
        "key = os.getenv(\"AZURE_COGNITIVE_SEARCH_KEY\")\n",
        "credential = AzureKeyCredential(key)\n",
        "index_name = os.getenv(\"AZURE_COGNITIVE_SEARCH_INDEX_NAME\")\n",
        "\n",
        "index_client = SearchIndexClient(\n",
        "    endpoint=service_endpoint, credential=credential)\n",
        "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f0501d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a pandas dataframe with columns from qa_set\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df = pd.DataFrame(qa_set)\n",
        "df = df.rename(columns={\"answer\": \"truth_answer\"})\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e794a118-188b-4e70-b0f0-3376146d077c",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Let's retrieve the relevant articles for each question in our qa_set dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93f48c97",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the articles for the search terms\n",
        "# Optional: change `num_docs` to change how many relevant ranked documents the Search index should return\n",
        "num_docs=1\n",
        "for i, row in df.iterrows():\n",
        "    search_term = row['question']\n",
        "    results = search_client.search(search_text=search_term, include_total_count=num_docs)\n",
        "    df.loc[i, \"context\"] = next(results)['article']\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c172dfb-a2fa-48b2-b723-96d6d521d7e8",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Using a prompt template, we can feed questions into GPT using the information from the retrieved documents.\n",
        "\n",
        "Notice which model we're now using to generate answers. Why might this be? What happens if you used the chat model we've used earlier?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31713b2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Ask the model using the embeddings from Challenges 3 and 4 to answer the questions\n",
        "template = \"\"\"You are a search assistant trying to answer the following question. Use only the context given. Your answer should only be one sentence.\n",
        "\n",
        "    > Question: {question}\n",
        "    \n",
        "    > Context: {context}\"\"\"\n",
        "\n",
        "# Create a prompt template\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
        "llm = AzureOpenAI(deployment_name=CHAT_INSTRUCT_MODEL, temperature=0)\n",
        "search_chain = LLMChain(llm=llm, prompt=prompt, verbose=False)\n",
        "\n",
        "prompt_answers = []\n",
        "for question, context in list(zip(df.question, df.context)):\n",
        "    response = search_chain.run(question=question, context=context)\n",
        "    prompt_answers.append(response.replace('\\n',''))\n",
        "df['prompt_answer'] = prompt_answers   "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252b880b",
      "metadata": {},
      "source": [
        "Examine the first three answers from the model based on the articles. How could you utilize Prompt Engineering techniques to refine the answers?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569b27de",
      "metadata": {},
      "outputs": [],
      "source": [
        "df['prompt_answer'].head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e340641-3ffd-4ea0-a35d-5a684b17d40d",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "After generating responses to our test questions, we can use GPT (can be another model if you would like, such as GPT 4) to evaluate the correctness to our Ground Truth answers using a rubric.\n",
        "\n",
        "Notice how the prompt is using techniques you learned from Challenges 1 and 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0583718",
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_template = \"\"\"You are trying to answer the following question from the context provided:\n",
        "\n",
        "> Question: {question}\n",
        "\n",
        "The correct answer is:\n",
        "\n",
        "> Query: {truth_answer}\n",
        "\n",
        "Is the following predicted query semantically the same (eg likely to produce the same answer)?\n",
        "\n",
        "> Predicted Query: {prompt_answer}\n",
        "\n",
        "Please give the Predicted Query a grade of either an A, B, C, D, or F, along with an explanation of why. End the evaluation with 'Final Grade: <the letter>'\n",
        "\n",
        "> Explanation: Let's think step by step.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba357c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_prompt = PromptTemplate(template=eval_template, input_variables=[\"question\", \"truth_answer\", \"prompt_answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8062ada",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new LLM Chain to submit the prompt we created\n",
        "eval_chain = LLMChain(llm=llm, prompt=eval_prompt, verbose=False)\n",
        "\n",
        "# Submit the prompt using our dataset\n",
        "eval_results = []\n",
        "for question, truth_answer, prompt_answer in list(zip(df.question, df.truth_answer, df.prompt_answer)):\n",
        "    eval_output = eval_chain.run(\n",
        "        question=question,\n",
        "        truth_answer=truth_answer,\n",
        "        prompt_answer=prompt_answer,\n",
        "    )\n",
        "    eval_results.append(eval_output)\n",
        "eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0dabe6a-659a-44ca-bc82-ac8793f713ef",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now let's parse the rubric results in order to quantify and summarize them in aggregate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8449fb78",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List\n",
        "from collections import defaultdict\n",
        "\n",
        "# Parse the evaluation chain responses into a rubric\n",
        "def parse_eval_results(results: List[str]) -> List[float]:\n",
        "    rubric = {\n",
        "        \"A\": 1.0,\n",
        "        \"B\": 0.75,\n",
        "        \"C\": 0.5,\n",
        "        \"D\": 0.25,\n",
        "        \"F\": 0\n",
        "    }\n",
        "    return [rubric[re.search(r'Final Grade: (\\w+)', res).group(1)] for res in results]\n",
        "\n",
        "scores = defaultdict(list)\n",
        "parsed_results = parse_eval_results(eval_results)\n",
        "\n",
        "# Collect the scores for a final evaluation table\n",
        "scores['request_synthesizer'].extend(parsed_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2090296",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reusing the rubric from above, parse the evaluation chain responses\n",
        "parsed_eval_results = parse_eval_results(eval_results)\n",
        "# Collect the scores for a final evaluation table\n",
        "scores['result_synthesizer'].extend(parsed_eval_results)\n",
        "\n",
        "# Print out Score statistics for the evaluation session\n",
        "header = \"{:<20}\\t{:<10}\\t{:<10}\\t{:<10}\".format(\"Metric\", \"Min\", \"Mean\", \"Max\")\n",
        "print(header)\n",
        "for metric, metric_scores in scores.items():\n",
        "    mean_scores = sum(metric_scores) / len(metric_scores) if len(metric_scores) > 0 else float('nan')\n",
        "    row = \"{:<20}\\t{:<10.2f}\\t{:<10.2f}\\t{:<10.2f}\".format(metric, min(metric_scores), mean_scores, max(metric_scores))\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a055d128-672d-4bd7-83cf-544ad9b6a803",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "There you have it! We can now review the results of evaluating the model in conjunction with Azure Cognitive Search from the last challenge. You can perform a similar analysis on your use case and custom data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0b743a96",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461ee50f-c2c4-4a80-bc0e-68fad17ee380",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "In this challenge, we covered the principles of Responsible AI, particularly when working with OpenAI, and how to evaluate the performance of a model implementation using Ground Truth data.\n",
        "\n",
        "We introduced you to several tools and services, some from Azure and others that are Open-Source. You can refer to them for your own projects to decide which works best for your scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718b928c-6041-4aba-9d65-23b493fbf84c",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Knowledge Check #1 Answers**:\n",
        "* True\n",
        "* False - it will be returned if it was not deemed inappropriate\n",
        "* False - your request will still complete without content filtering. You can see if it wasn't applied by looking for an error message in the `content_filter_result` object."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a9fe7e6-4a36-4627-9df5-fe936f9dd93b",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Knowledge Check #2 Answers**:\n",
        "* False: the service was trained on more than 100 languages but is designed to support only a handful.\n",
        "* True: Content Safety has a monitoring page to help you track you moderation API performance and trends to inform your content moderation strategy.\n",
        "* True: The Studio uses four levels of risk, whereas the API scores the risk on a scale of 0 to 6.\n",
        "* False: You can also customize severity thresholds in the Studio.\n",
        "* False: You can specify which categories you want to assess your text on in the API using the `categories` parameter."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
