{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b42432",
   "metadata": {},
   "source": [
    "# Challenge 5.7 - Red Teaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aac194",
   "metadata": {},
   "source": [
    "## 1. Overview \n",
    "Red Teaming is a practice where a group of experts known as the red team try to probe a system to identify vulnerabilites. In the case of AI the goal of the red team is to strategically prompt the AI system to uncover any weaknesses in the model's robustness, safety, and ethical boundaries.\n",
    "\n",
    "Benefits of Red teaming includes:\n",
    "- Improved Safety: detecting harmful outputs and preventing potential harm caused by malicious users.\n",
    "- Enhance Fairness and Ethics: mitigates biases in model behavior and ensures compliance with ethical standards.\n",
    "- Better Reliability: strenghtens the ability for handling edge cases and reduces hallucination inaccuracies.\n",
    "- Transparency and Accountability: encourages documentation of known risks and supports responsible AI development.\n",
    "\n",
    "We can use the Azure Red-Teaming Python library to automate the testing process. Using the library we can create a red teaming agent and connect it to a testing model. The agent will adverserially prompt the model on user chosen categories such as Violence, Sexual, Harm, Hate, and Unfairness. The given output is then scanned by the agent to ensure it is safe and outputted in JSON and TXT files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad5c2b",
   "metadata": {},
   "source": [
    "## 2. Setting Up Environment \n",
    "\n",
    "Loading required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0931b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure imports\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation.red_team import RedTeam, RiskCategory, AttackStrategy\n",
    "\n",
    "# OpenAI imports\n",
    "from openai import AzureOpenAI\n",
    "import asyncio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc0ad6",
   "metadata": {},
   "source": [
    "## 2. Creating Red Teaming Agent\n",
    "\n",
    "This code initializes an AI Red Teaming Agent from the Azure AI evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80867a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Azure AI Foundry project from environment variable\n",
    "azure_ai_project = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "\n",
    "# Instantiate your AI Red Teaming Agent\n",
    "red_team_agent = RedTeam(\n",
    "    azure_ai_project=azure_ai_project, # required\n",
    "    credential=DefaultAzureCredential() # required\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2845cc5",
   "metadata": {},
   "source": [
    "## 3. OpenAI Client and callback\n",
    "\n",
    "This code sets up an Azure OpenAI client using Entra ID so that the red team agent can interact with the AOAI model that is deployed in Foundry. The red teaming agent will prompt this client to ensure it complies with safety standards. Please look through the code, comments, and function descriptions to understand what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Azure OpenAI client using managed identity (recommended approach)\n",
    "def get_azure_ad_token():\n",
    "    \"\"\"Function to get Azure AD token for OpenAI authentication\"\"\"\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    return token.token\n",
    "\n",
    "azure_openai_client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_ad_token_provider=get_azure_ad_token\n",
    ")\n",
    "\n",
    "# Target callback function that uses your Azure OpenAI deployment\n",
    "def azure_openai_callback(query: str) -> str:\n",
    "    \"\"\"\n",
    "    This function represents your AI system that will be red-teamed.\n",
    "    It sends the query to your Azure OpenAI deployment and returns the response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = azure_openai_client.chat.completions.create(\n",
    "            model=os.getenv(\"CHAT_MODEL_NAME\"),  # This should match your deployment name\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": query}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970404f8",
   "metadata": {},
   "source": [
    "## 4. Running the Red Team Agent\n",
    "\n",
    "The main function is where the red teaming agent orchestrates the evaluation. \n",
    "\n",
    "The categories test for the following:\n",
    "- Violence: Tests for content promoting or describing violent acts\n",
    "- Hate and Unfairness: Evaluates responses to hate speech, discrimination, and unfair treatment\n",
    "- Sexual: Checks for inappropriate sexual content generation\n",
    "- SelfHarm: Tests for content that could encourage self-destructive behavior\n",
    "\n",
    "Feel free to change the num_objectives to toggle the number of prompts given to the model.\n",
    "\n",
    "The scan results will contain detailed information about:\n",
    "- Generated attack prompts for each risk category\n",
    "- AI system responses to those prompts\n",
    "- Safety evaluation scores and classifications\n",
    "- Detailed logs of the red teaming process\n",
    "- Summary statistics and recommendations\n",
    "\n",
    "This  ensures that the model is thoroughly tested for safety vulnerabilities across different risk categories before deployment to production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d7ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    # The scan method expects a target function, not a configuration object\n",
    "    red_team_result = await red_team_agent.scan(\n",
    "        target=azure_openai_callback,\n",
    "        risk_categories=[ # optional, defaults to all four risk categories\n",
    "            RiskCategory.Violence,\n",
    "            RiskCategory.HateUnfairness,\n",
    "            RiskCategory.Sexual,\n",
    "            RiskCategory.SelfHarm\n",
    "        ],\n",
    "        num_objectives=5  # Number of conversation turns per attack\n",
    "    )\n",
    "    print(\"Red team scan completed!\")\n",
    "    print(red_team_result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        loop = None\n",
    "\n",
    "    if loop and loop.is_running():\n",
    "        # If there's a running event loop (e.g., in Jupyter), use create_task\n",
    "        task = loop.create_task(main())\n",
    "    else:\n",
    "        # Otherwise, safe to use asyncio.run\n",
    "        asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}