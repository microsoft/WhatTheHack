{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 04-B - Retrieval Augmented Generation (RAG) for Unstructured Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Businesses have a lot of proprietary information that needs to be taken into account when answering user's questions - these cannot always be answered through the data that the GPT models have been trained on. \n",
    "\n",
    "In the last notebook, we worked with structured data primarily. A lot of the time, your enterprise data is not limited to just structured formats like CSV files or SQL tables. It may also include unstructured data like PDF documents or images. In fact, your individual documents could have both unstructured and structured data built into them. Extracting information from these diverse formats in a comprehensible manner presents a challenge. Tools like Azure Form Recognizer enable the extraction of data from unstructured sources such as forms or documents. Once the data is extracted into a structured JSON format, then Cognitive Search can be utilized to consolidate the entire information from different data types into indexes, facilitating the retrieval of relevant documents.\n",
    "\n",
    "In this notebook, we will walk you through a use case of Retrieval Augmented Generation (RAG) that involves working with unstructured data. The RAG approach combines various technologies to enhance the quality and relevance of generated outputs. We will leverage Azure Form Recognizer to process complex documents, utilizing the layout API to extract text and tables effectively. We will utilize Azure Cognitive Search to create an index by configuring semantic search capabilities, enabling the retrieval of relevant document pages. Additionally, embeddings will be incorporated to retrieve content that is more closely aligned with the user's question. Finally, Azure OpenAI's ChatGPT model will utilize the extracted content to generate a more meaningful answer. It is important to emphasize that this grounding process follows the RAG pattern mentioned in the previous notebook and helps eliminate inaccuracies in the generated responses.\n",
    "\n",
    "Your goals for this challenge are to read through this notebook, run each code block, observe the results, and then be able to answer the questions posed in the student guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Azure Forms Recognizer, Azure Cognitive Search, OpenAI, and other python modules\n",
    "\n",
    "import os, json, requests, sys, re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is secure and recommended way to load OpenAI resource credentials and deployment names\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model=os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The path in the code cell below is referring to the `/data/unstructured/raw` folder of the `Resources.zip` file. You may need to update this path if you are running this notebook from a different location then from where you extracted it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- raw data\n",
    "RAW_DATA_FOLDER= '../data/unstructured/raw'\n",
    "# -- extracted json file \n",
    "EXTRACTED_DATA_FOLDER = '../data/unstructured/extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract the data from our unstructured data into a more readable format for the model to understand. The Form Recognizer tool helps us do so by leveraging the prebuilt layout models. Here, we primarily are working with PDFs but we could also have JPG and PNG formats that the form recognizer tool also supports.\n",
    "\n",
    "For each document, we want to specify the way information is being extracted. For example in this use case, each document has many pages. To keep track of the pages, we store them in page_number. We also want to extract the content for each page and drop it in a page_context field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_single_file(file_name: str):\n",
    "    not_completed = True\n",
    "    while not_completed:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            not_completed=False\n",
    "    result = poller.result()\n",
    "    return get_page_content(file_name, result)\n",
    "\n",
    "def extract_files( folder_name: str, destination_folder_name: str):\n",
    "    os.makedirs(destination_folder_name, exist_ok=True)\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file[-3:].upper() in ['PDF','JPG','PNG']:\n",
    "            print('Processing file:', file, end='')\n",
    "        \n",
    "            page_content = extract_local_single_file(os.path.join(folder_name, file))\n",
    "            output_file = os.path.join(destination_folder_name, file[:-3] +'json')\n",
    "            print(f'  write output to {output_file}')\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(json.dumps(page_content))\n",
    "\n",
    "\n",
    "def get_page_content(file_name:str, result):\n",
    "    page_content = []\n",
    "    for page in result.pages:\n",
    "        all_lines_content = []\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            all_lines_content.append(' '.join([word.content for word in line.get_words()]))\n",
    "        page_content.append({'page_number':page.page_number, \n",
    "                                'page_content':' '.join(all_lines_content)})\n",
    "    return {'filename':file_name, 'content':page_content}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf  write output to ./data/unstructured/extracted\\AutoPrompt_Eliciting_Knowledge_From_LanguageModels.json\n",
      "Processing file: Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.pdf  write output to ./data/unstructured/extracted\\Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.json\n",
      "Processing file: Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf  write output to ./data/unstructured/extracted\\Generated_Knowledge_Prompting_for_Commonsense_Reasoning.json\n",
      "Processing file: LLMs_are_Human-Level_Prompt_Engineers.pdf  write output to ./data/unstructured/extracted\\LLMs_are_Human-Level_Prompt_Engineers.json\n",
      "Processing file: Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf  write output to ./data/unstructured/extracted\\Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.json\n",
      "Processing file: Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf  write output to ./data/unstructured/extracted\\Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.json\n",
      "Processing file: Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf  write output to ./data/unstructured/extracted\\Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.json\n",
      "Processing file: Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.pdf  write output to ./data/unstructured/extracted\\Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.json\n"
     ]
    }
   ],
   "source": [
    "extract_files(RAW_DATA_FOLDER, EXTRACTED_DATA_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More About our data\n",
    "\n",
    "For this walkthrough, we will take a look at various Research Papers on LLM topics in PDF documents. This includes topics like autoprompting, chain of thought prompting, precise zero shot dense retrival, and more. This dataset contains various unstructured formats such as text, tables, graphs, and formulas.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The relevant schema for our work today consists of \n",
    "\n",
    "- document_id\n",
    "- document_name\n",
    "- file_path\n",
    "- page_number\n",
    "- page_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for file in os.listdir(EXTRACTED_DATA_FOLDER):\n",
    "    with open(os.path.join(EXTRACTED_DATA_FOLDER, file)) as f:\n",
    "        page_content= json.loads(f.read())\n",
    "    documents.extend(\n",
    "        [\n",
    "            {\n",
    "                'document_id':page_content['filename'].split('\\\\')[-1].split('.')[0] + '-' + str(page['page_number']),\n",
    "                'document_name':page_content['filename'].split('\\\\')[-1],\n",
    "                'file_path':page_content['filename'],              \n",
    "                'page_number':page['page_number'],\n",
    "                'page_text':page['page_content']\n",
    "            }\n",
    "            for page in page_content['content']\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': 'AutoPrompt_Eliciting_Knowledge_From_LanguageModels-2',\n",
       " 'document_name': 'AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf',\n",
       " 'file_path': './data/unstructured/raw\\\\AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf',\n",
       " 'page_number': 2,\n",
       " 'page_text': 'Original Input xinp a real joy. AUTOPROMPT Xprompt a real joy. atmosphere alot dialogue Clone totally [MASK]. Trigger Tokens Œtrig atmosphere, alot, dialogue, Clone ... Masked LM p([MASK] | @prompt) Cris marvelous philanthrop Template (xinp, \"trig) {sentence }[T][T][T][T][T][P]. p(y|xprompt ) + positive worse incompetence Worse + negative Figure 1: Illustration of AUTOPROMPT applied to probe a masked language model\\'s (MLM\\'s) ability to per- form sentiment analysis. Each input, «inp, is placed into a natural language prompt, «prompt, which contains a single [MASK] token. The prompt is created using a template, A, which combines the original input with a set of trigger tokens, «trig. The trigger tokens are shared across all inputs and determined using a gradient-based search (Section 2.2). Probabilities for each class label, y, are then obtained by marginalizing the MLM predictions, p([MASK]|@prompt), over sets of automatically detected label tokens (Section 2.3). vides a lower bound on what the model \"knows\", and is therefore a more useful analysis tool. How- ever, prompting unfortunately requires manually crafting the context to feed into the model. Not only is this time consuming and non-intuitive for many tasks (e.g., textual entailment), more impor- tantly, models are highly sensitive to this context: improperly-constructed contexts cause artificially low performance (Jiang et al., 2020). Overcoming the need to manually specify prompts would make prompting a more widely useful analysis tool. In this paper, we introduce AUTOPROMPT-an automated method for generating prompts for any task, illustrated in Figure 1. Given a task, e.g., sen- timent analysis, AUTOPROMPT creates a prompt by combining the original task inputs (e.g. reviews) with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in Wallace et al. (2019). The LM predictions for the prompt are converted to class probabilities by marginal- izing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier. We validate the effectiveness of AUTOPROMPT in numerous experiments. First, we use AUTO- PROMPT to construct prompts that test pretrained masked language models (MLMs) on sentiment analysis and natural language inference (NLI). Our tests reveal that, without any finetuning, MLMs perform well on both of these tasks-a properly- prompted ROBERTa achieves 91% accuracy on SST-2 (better than a finetuned ELMo model (Pe- ters et al., 2018)), and 69% accuracy on a bal- anced variant of the SICK-E dataset (Marelli et al., 2014). Next, we apply AUTOPROMPT to the fact re- trieval tasks of LAMA (Petroni et al., 2019), where we are able to construct prompts that more effec- tively elicit MLM\\'s factual knowledge than exist- ing prompts generated using manual and corpus- mining methods. Concretely, we achieve 43.3% precision-at-1, compared to the current best single- prompt result of 34.1% (Jiang et al., 2020). We also introduce a variant of this task, similar to rela- tion extraction (RE), that tests whether MLMs can extract knowledge from a given piece of text. We show that MLMs can actually outperform existing RE models when context sentences with real facts are provided, however, they struggle when context sentences are artificially falsified. Finally, although the goal of AUTOPROMPT is to analyze models, we find that it provides certain practical advantages over finetuning. First, AU- TOPROMPT achieves higher average- and worst- case accuracy than finetuning in low-data regimes. Moreover, unlike finetuning, prompting LMs does not require large amounts of disk space to store model checkpoints; once a prompt is found, it can be used on off-the-shelf pretrained LMs. This is beneficial when serving models for multiple tasks. 2 Overview of AUTOPROMPT A natural way to elicit knowledge from pretrained LMs is to pose tasks as fill-in-the-blank problems.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a single page of research paper file that will be indexed in Azure Cognitive Search\n",
    "documents[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will focus on Cognitive Search and the following topics:\n",
    "1. Creating an index client\n",
    "2. Defining the index fields with necessary attributes\n",
    "3. Creating a semantic configuration\n",
    "4. Loading our index with the document pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.search.documents.indexes._search_index_client.SearchIndexClient at 0x2f357dc9cd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SDK client\n",
    "# service_endpoint = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\")   \n",
    "key = os.getenv(\"AZURE_COGNITIVE_SEARCH_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_name = \"research-paper-index\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " research-paper-index created\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"document_id\", type=SearchFieldDataType.String, key=True),\n",
    "    SimpleField(name=\"page_number\", type=SearchFieldDataType.Int64),\n",
    "    SimpleField(name=\"file_path\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"document_name\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"page_text\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"document_id\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"document_name\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"page_text\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 179 documents\n"
     ]
    }
   ],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see Azure Cognitive Search in action! We can retrive the most relevant documents out of all the ones that we are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is automated prompt engineering?\"\n",
    "count = 10\n",
    "results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "page_chunks = []\n",
    "citations = []\n",
    "for result in results:\n",
    "    page_chunks.append(result['page_text'])\n",
    "    citations.append(result['document_name'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD Large models ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTOPROMPT: Eliciting Knowledge from Language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question Tracy used a piece of wire 4 feet lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Instruction Only In-context Only Instruction +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Original Input xinp a real joy. AUTOPROMPT Xpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&gt; p([MASK] = w| @prompt) WEVy However, writing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Table 24: Few-shot exemplars for full chain of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Task CSQA2 Prompt Generate some knowledge abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Task NumerSense Prompt Generate some numerical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks\n",
       "0  A PROMPT ENGINEERING IN THE WILD Large models ...\n",
       "1  LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...\n",
       "2  AUTOPROMPT: Eliciting Knowledge from Language ...\n",
       "3  Question Tracy used a piece of wire 4 feet lon...\n",
       "4  Instruction Only In-context Only Instruction +...\n",
       "5  Original Input xinp a real joy. AUTOPROMPT Xpr...\n",
       "6  > p([MASK] = w| @prompt) WEVy However, writing...\n",
       "7  Table 24: Few-shot exemplars for full chain of...\n",
       "8  Task CSQA2 Prompt Generate some knowledge abou...\n",
       "9  Task NumerSense Prompt Generate some numerical..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df = pd.DataFrame(page_chunks, columns = [\"page_chunks\"]) #datframe with document chunks\n",
    "embed_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the most relevant documents, let us create embeddings for all the page chunks. This will help us find the most similar documents to our given user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Rate Limits\n",
    "\n",
    "from openai.error import RateLimitError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def get_embedding(text: str, engine: str = \"text-embedding-ada-002\"):\n",
    "    count=0\n",
    "    while True:\n",
    "        try:\n",
    "            embedding = openai.Embedding().create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
    "            break;\n",
    "        except RateLimitError:\n",
    "            count+=1\n",
    "            #print(f'RateLimitError Count: {count}')\n",
    "            sleep(2)            \n",
    "    return np.array(embedding).astype(np.float32)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\"): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD Large models ...</td>\n",
       "      <td>[-0.024409005, 0.0011260926, 0.006095364, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...</td>\n",
       "      <td>[-0.02069425, -0.003942089, 0.00025089542, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTOPROMPT: Eliciting Knowledge from Language ...</td>\n",
       "      <td>[-0.019387873, 0.0026623704, 0.0016202723, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question Tracy used a piece of wire 4 feet lon...</td>\n",
       "      <td>[0.012753163, 0.006085661, 0.015384983, -0.032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Instruction Only In-context Only Instruction +...</td>\n",
       "      <td>[-0.023519488, 0.011074509, 0.017816115, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Original Input xinp a real joy. AUTOPROMPT Xpr...</td>\n",
       "      <td>[-0.027597042, -0.014452211, 0.0037054217, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&gt; p([MASK] = w| @prompt) WEVy However, writing...</td>\n",
       "      <td>[-0.039878074, -0.0032404505, -0.0033636412, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Table 24: Few-shot exemplars for full chain of...</td>\n",
       "      <td>[0.013435593, 0.012619638, 0.033219036, -0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Task CSQA2 Prompt Generate some knowledge abou...</td>\n",
       "      <td>[0.017375462, 0.031131035, 0.036393795, -0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Task NumerSense Prompt Generate some numerical...</td>\n",
       "      <td>[-0.0027567996, 0.012855793, 0.03290082, -0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks  \\\n",
       "0  A PROMPT ENGINEERING IN THE WILD Large models ...   \n",
       "1  LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...   \n",
       "2  AUTOPROMPT: Eliciting Knowledge from Language ...   \n",
       "3  Question Tracy used a piece of wire 4 feet lon...   \n",
       "4  Instruction Only In-context Only Instruction +...   \n",
       "5  Original Input xinp a real joy. AUTOPROMPT Xpr...   \n",
       "6  > p([MASK] = w| @prompt) WEVy However, writing...   \n",
       "7  Table 24: Few-shot exemplars for full chain of...   \n",
       "8  Task CSQA2 Prompt Generate some knowledge abou...   \n",
       "9  Task NumerSense Prompt Generate some numerical...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.024409005, 0.0011260926, 0.006095364, -0.0...  \n",
       "1  [-0.02069425, -0.003942089, 0.00025089542, -0....  \n",
       "2  [-0.019387873, 0.0026623704, 0.0016202723, -0....  \n",
       "3  [0.012753163, 0.006085661, 0.015384983, -0.032...  \n",
       "4  [-0.023519488, 0.011074509, 0.017816115, -0.00...  \n",
       "5  [-0.027597042, -0.014452211, 0.0037054217, -0....  \n",
       "6  [-0.039878074, -0.0032404505, -0.0033636412, -...  \n",
       "7  [0.013435593, 0.012619638, 0.033219036, -0.006...  \n",
       "8  [0.017375462, 0.031131035, 0.036393795, -0.005...  \n",
       "9  [-0.0027567996, 0.012855793, 0.03290082, -0.00...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD Large models ...</td>\n",
       "      <td>[-0.024409005, 0.0011260926, 0.006095364, -0.0...</td>\n",
       "      <td>0.891098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...</td>\n",
       "      <td>[-0.02069425, -0.003942089, 0.00025089542, -0....</td>\n",
       "      <td>0.866633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTOPROMPT: Eliciting Knowledge from Language ...</td>\n",
       "      <td>[-0.019387873, 0.0026623704, 0.0016202723, -0....</td>\n",
       "      <td>0.841785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks  \\\n",
       "0  A PROMPT ENGINEERING IN THE WILD Large models ...   \n",
       "1  LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...   \n",
       "2  AUTOPROMPT: Eliciting Knowledge from Language ...   \n",
       "\n",
       "                                           embedding  similarities  \n",
       "0  [-0.024409005, 0.0011260926, 0.006095364, -0.0...      0.891098  \n",
       "1  [-0.02069425, -0.003942089, 0.00025089542, -0....      0.866633  \n",
       "2  [-0.019387873, 0.0026623704, 0.0016202723, -0....      0.841785  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = get_embedding(query, engine=embedding_model)\n",
    "embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "top_results = (\n",
    "    embed_df.sort_values(\"similarities\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    "    .head(3)\n",
    ")\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
      "Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
      "\n",
      "User Query: ```What is automated prompt engineering?```\n",
      "List of Extracted Pages: ```['A PROMPT ENGINEERING IN THE WILD Large models with natural language interfaces, including models for text generation and image synthesis, have seen an increasing amount of public usage in recent years. As finding the right prompt can be difficult for humans, a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Among others, see, for example: · https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/ . https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/ . https://news.ycombinator.com/item?id=32943224 . https://promptomania.com/stable-diffusion-prompt-builder/ . https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion In this paper we apply APE to generate effective instructions for steering LLMs, but the general framework Algorithm 1 could be applied to steer other models with natural language interfaces so long as an appropriate proposal method and scoring function can be designed. 15', 'LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUMAN-LEVEL Yongchao Zhou*,1,2 Andrei Ioan Muresanu*,2,3 Ziwen Han*,1,2 Keiran Paster1,2 Silviu Pitis1,2 Harris Chan1,2 Jimmy Ba1,2 1 University of Toronto 2 Vector Institute 3 University of Waterloo ABSTRACT By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the \"program,\" optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer. arXiv:2211.01910v1 [cs.LG] 3 Nov 2022 1 INTRODUCTION The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called \"large language models\" (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do? To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin & Eisner, 2021; Lester et al., 2021) and natural language prompt engineering (Reynolds & McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples). Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task (Sanh et al., 2022; Wei et al., 2021). To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem Equal contribution. Corresponding email: yczhou@cs.toronto.edu 1 Our code is available at https : //github. com/keirp/automatic_prompt_engineer. 1', 'AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts Taylor Shin* Yasaman Razeghi *< Eric Wallace University of California, Irvine Robert L. Logan IV* Sameer Singh *University of California, Berkeley {tshin1, yrazeghi, rlogan, sameer } @uci.edu ericwallace@berkeley.edu Abstract The remarkable success of pretrained lan- guage models has motivated the study of what kinds of knowledge these models learn dur- ing pretraining. Reformulating tasks as fill- in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suit- able prompts. To address this, we develop AUTOPROMPT, an automated method to cre- ate prompts for a diverse set of tasks, based on a gradient-guided search. Using AUTO- PROMPT, we show that masked language mod- els (MLMs) have an inherent capability to perform sentiment analysis and natural lan- guage inference without additional parame- ters or finetuning, sometimes achieving per- formance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowl- edge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that au- tomatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a re- placement for finetuning. arXiv:2010.15980v2 [cs.CL] 7 Nov 2020 1 Introduction Pretrained language models (LMs) have had ex- ceptional success when adapted to downstream tasks via finetuning (Peters et al., 2018; Devlin et al., 2019). Although it is clear that pretrain- ing improves accuracy, it is difficult to determine whether the knowledge that finetuned LMs contain is learned during the pretraining or the finetuning * First three authors contributed equally. process. How can we directly evaluate the knowl- edge present in pretrained LMs, be it linguistic, factual, commonsense, or task-specific? Numerous techniques have been proposed to elicit such knowledge by analyzing pretrained LMs\\' internal representations. A common strategy is to use probing classifiers-shallow classifiers that predict certain attributes using an LMs\\' representa- tions as features (Conneau et al., 2018; Liu et al., 2019). However, probing classifiers require ad- ditional learned parameters and are thus suscep- tible to false positives; high probing accuracy is not a sufficient condition to conclude that an LM contains a certain piece of knowledge (Hewitt and Liang, 2019; Voita and Titov, 2020). Attention visualization, another common technique, has a similar failure mode: attention scores may be corre- lated with, but not caused by the underlying target knowledge, leading to criticism against their use as explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Both probing and attention visu- alizations also struggle to evaluate knowledge that cannot be represented as simple token- or sequence- level classification tasks. A more direct approach for eliciting knowledge from these models, since they are language models after all, is prompting, i.e. converting tasks into a language model format. For example, Radford et al. (2019) frame summarization as a language modeling task by appending \"TL;DR:\" to the end of an article and then generating from an LM. Sim- ilarly, Petroni et al. (2019) manually reformulate a knowledge base completion task as a cloze test (i.e., a fill-in-the-blank problem). Compared to existing model analysis methods, prompting is non- invasive: it does not introduce large amounts of additional parameters or require direct inspection of a model\\'s representations. Thus prompting pro- 1']```\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "User Query: ```{query}```\n",
    "List of Extracted Pages: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated prompt engineering refers to the process of automatically generating natural language instructions or prompts to steer large language models (LLMs) towards desired behaviors. This is done by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. The quality of the selected instruction is evaluated by evaluating the zero-shot performance of another LLM following the selected instruction. A number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Some examples of these tools include Promptomania, Stable Diffusion Prompt Builder, and MagicPrompt. The goal of automated prompt engineering is to reduce the human effort involved in creating and validating effective instructions, and to improve the performance of LLMs on downstream tasks.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt, chat_model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_search(query, count=10):\n",
    "    results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "    page_chunks = []\n",
    "    for result in results:\n",
    "        page_chunks.append(result['page_text'])\n",
    "        \n",
    "    #Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "    embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))\n",
    "\n",
    "    query_embedding = get_embedding(query, engine=embedding_model)\n",
    "    embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "    top_results = (\n",
    "        embed_df.sort_values(\"similarities\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .head(3)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "    Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "    User Query: ```{query}```\n",
    "    List of Extracted Pages: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated prompt engineering involves using natural language instructions to steer large language models (LLMs) towards desired behaviors. This is achieved through the generation and selection of effective prompts, which can be difficult for humans to find. To address this, a number of guides and tools have been developed, including the Automatic Prompt Engineer (APE) algorithm, which uses LLMs to automatically generate and select instructions. APE has been shown to outperform prior LLM baselines and achieve better or comparable performance to instructions generated by human annotators on 19/24 tasks. Another automated method for creating prompts is AUTOPROMPT, which uses a gradient-guided search to create prompts for a diverse set of tasks. Using AUTOPROMPT, it has been shown that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. Additionally, prompts generated by AUTOPROMPT elicit more accurate factual knowledge from MLMs than manually created prompts on the LAMA benchmark, and MLMs can be used as relation extractors more effectively than supervised relation extraction models. Overall, automated prompt engineering involves using algorithms to generate effective prompts for LLMs and has shown promising results in improving task performance.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"How does automated prompt engineering work?\", 5)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tuning refers to the process of finding the right prompt for natural language interfaces, including models for text generation and image synthesis. It can be difficult for humans to find the right prompt, so a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. AUTOPROMPT is an automated method for generating prompts for any task, illustrated in Figure 1. Given a task, e.g., sentiment analysis, AUTOPROMPT creates a prompt by combining the original task inputs (e.g. reviews) with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in Wallace et al. (2019). The LM predictions for the prompt are converted to class probabilities by marginalizing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"what is prompt tuning?\", 10)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaiworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
