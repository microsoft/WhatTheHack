{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61ce4fd-c422-4379-b3ef-f30345cdbf79",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Spark Data Engineering Solution\n",
    "\n",
    "This is a complete solution to the data engineering aspects of the hack best performed by Python/Spark. \n",
    "\n",
    "**Note** XML forecast data provided by the BOM is far easier parsed and persisted by a dataflow2 than in code so not included here. \n",
    "\n",
    "Students can start from scratch and land data from the source, or they can start with raw, slightly cleaned (bronze), or merged (silver) data as contained in ``resources.zip``\n",
    "\n",
    "The final objective is to have an enriched 'Silver' zone table of shipwrecks, with their correct marine zones added.\n",
    "\n",
    "Be aware, that Parquet does not yet support the ``geometry`` type. Instead of mucking about with WKT, we're using geojson for our Bronze zone.\n",
    "\n",
    "**Exploring the Data**\n",
    "\n",
    "A few ideas on how students might want to explore the data along the way can be found in the notebook ``Data Exploration``. Students will probably need to at least perform a cursory glance at the data before writing to Delta, and this notebook has some pretty charts you can use to demo the exploratory process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac01ae-7fb8-4f95-b396-a1701b0f93f1",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Common Code\n",
    "This section contains some code common to both Challenge 2 and Challenge 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc64784c-211f-456b-88a4-dfc79e05b3ad",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Install Dependencies\n",
    "\n",
    "geopandas is needed to work with geojson files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f5aea-f95a-424f-92a9-443e133fe5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585848e7-4bb5-47c1-9098-a4d516e32507",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Setup Imports & Folder/File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bf5f6-b9ec-4992-b6a8-c54a80bd7c1c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "# Standard zones (gold can be considered the semantic (power bi dataset) model)\n",
    "rawFilesFolder = \"/lakehouse/default/Files/Raw/\"\n",
    "bronzeFilesFolder = \"/lakehouse/default/Files/Bronze/\"\n",
    "silverFilesFolder = \"/lakehouse/default/Files/Silver/\"\n",
    "\n",
    "\n",
    "# These files are created by createStudentResources.py \n",
    "shipwrecksBronzeFile = f\"{bronzeFilesFolder}shipwrecks.geojson\"\n",
    "marineZonesBronzeFile = f\"{bronzeFilesFolder}marinezones.geojson\"\n",
    "shipwrecksSilverFile = f\"{silverFilesFolder}shipwrecks.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628f3dd-38b7-43d3-a875-82a3620262f9",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Challenge 2 - Land Ho! - Some Handy Functions to download and unzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43f65e-77eb-4cda-94b5-f74ebe737a60",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "from re import sub\n",
    "\n",
    "def downloadFile(url, saveFolder):\n",
    "    filename = os.path.basename(url)\n",
    "    filepath = os.path.join(saveFolder, filename)\n",
    "    os.makedirs(saveFolder, exist_ok=True)\n",
    "    print(f\"Downloading file from URL: {url} to {filepath}\")\n",
    "    urllib.request.urlretrieve(url, filepath)\n",
    "    return(filepath)\n",
    "\n",
    "def unzipFile(zipfilePath, extractPath):\n",
    "    print(f\"Extracting {zipfilePath} to {extractPath}\")\n",
    "    with zipfile.ZipFile(zipfilePath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extractPath)\n",
    "    os.remove(zipfilePath)\n",
    "\n",
    "def toPascalCase(s):\n",
    "  s = sub(r\"(_|-)+\", \" \", s).title().replace(\" \", \"\").replace(\"*\",\"\")\n",
    "  return ''.join(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf77af0a-63ab-472b-b578-f6061eb259ad",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Challenge 2 Solutions\n",
    "\n",
    "Challenge 2 solutions are arranged per dataset - Shipwrecks, Marine Zones and Forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b2496f-e117-4d9f-8388-fe239dd4b138",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Shipwrecks Landing From Source - Preferred Github version\n",
    "\n",
    "A copy of ``Shipwrecks_WAM_002_WA_GDA94_Public.geojson`` is contained in the hack repo to allow easier access if students wish to attempt to download data themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff14ad0-bc02-4440-ac9d-e5cbe1930f0a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "shipwrecksFileUrl = \"https://raw.githubusercontent.com/liesel-h/WhatTheHack/xxx-FabricLakehouse/067-FabricLakehouse/Student/Resources/Shipwrecks_WAM_002_WA_GDA94_Public.geojson\"\n",
    "shipwrecksRawFile = downloadFile(shipwrecksFileUrl, shipwrecksRawFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53d5d7-bce3-4efc-9f52-abf81ceb19e7",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Shipwrecks Landing From Source - SLIP Version (Only for the advanced student)\n",
    "\n",
    "This is an example of importing ``Shipwrecks_WAM_002_WA_GDA94_Public_GeoJSON.zip`` from SLIP, provided for completeness. As this requires a SLIP login, a copy of this data is also provided in github, and students should probably use this mirror instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fde509-17b4-4700-8563-136acfb2a022",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def downloadSLIPFile(slipPath, slipFile, saveFolder, userId, password) :\n",
    "    \"\"\"\n",
    "    Downloads a file from West Australian government Shared Location Information Platform (SLIP)\n",
    "\n",
    "    Code based on https://toolkit.data.wa.gov.au/hc/en-gb/articles/115000962734 \n",
    "    \"\"\"\n",
    "\n",
    "    saveFile = f\"{saveFolder}/{slipFile}\"\n",
    "    os.makedirs(saveFile, exist_ok=True)\n",
    "    \n",
    "    dataDownloadRequestUrl = \"https://direct-download.slip.wa.gov.au/datadownload/{0}/{1}\".format(slipPath, slipFile)\n",
    "\n",
    "\n",
    "    tokenRequestUrl = \"https://sso.slip.wa.gov.au/as/token.oauth2\"\n",
    "    tokenRequestHeaders = { 'Authorization' : 'Basic ZGlyZWN0LWRvd25sb2Fk'}\n",
    "    tokenRequestForm={\"grant_type\": \"password\", \"username\":userId, \"password\":password}\n",
    "    tokenResponse = requests.post(tokenRequestUrl, data=tokenRequestForm, headers=tokenRequestHeaders)\n",
    "    accessToken=json.loads(tokenResponse.text)[\"access_token\"]\n",
    "\n",
    "    if tokenResponse.status_code == 200:\n",
    "        print(f\"Downloading file from URL: {dataDownloadRequestUrl} to {saveFolder}\")\n",
    "        dataDownloadRequestHeaders = { 'Authorization' : 'Bearer ' + accessToken}\n",
    "        dataDownloadResponse = requests.get(dataDownloadRequestUrl, headers=dataDownloadRequestHeaders)\n",
    "        if dataDownloadResponse.status_code == 200:\n",
    "            with open(saveFile, 'wb') as f:\n",
    "                f.write(dataDownloadResponse.content)\n",
    "            \n",
    "            with zipfile.ZipFile(saveFile, 'r') as zipref:\n",
    "                geojsonfile=[filename for filename in zipref.namelist() if filename.endswith('.geojson')][0]\n",
    "                zipref.extractall(saveFolder)\n",
    "                return f\"{saveFolder}/{geojsonfile}\"\n",
    "        else:\n",
    "            print(\"Error download file with error \" + str(dataDownloadResponse.status_code) + \"-\" + dataDownloadResponse.text)\n",
    "    else:\n",
    "        print(\"Error getting token: \" + str(tokenResponse.status_code) + \"-\" + tokenResponse.text)\n",
    "\n",
    "\n",
    "# SLIP username and password\n",
    "# Storing creds in a notebook is never a good idea, use Key Vault\n",
    "# mssparkutils.credentials.getSecret('https://SomeKeyVault.vault.azure.net/','SomeSecret')\n",
    "#\n",
    "# https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python\n",
    "\n",
    "SLIPUsername = \"\"\n",
    "SLIPPassword = \"\"\n",
    "\n",
    "#The WAM-002 Shipwrecks data https://direct-download.slip.wa.gov.au/datadownload/People_and_Society/Shipwrecks_WAM_002_WA_GDA94_Public_GeoJSON.zip\n",
    "SLIPFolder=\"People_and_Society\"\n",
    "SLIPFile=\"Shipwrecks_WAM_002_WA_GDA94_Public_GeoJSON.zip\"\n",
    "WAMsaveFolder = f\"{rawFilesFolder}WAM\"\n",
    "\n",
    "shipwrecksRawFile = downloadSLIPFile(SLIPFolder, SLIPFile, WAMsaveFolder, SLIPUsername, SLIPPassword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2916bd-3e69-424f-ae0c-74bd559aabba",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Shipwrecks Transforming Raw to Bronze\n",
    "\n",
    "Regardless of origin, the steps to transform the raw geojson to Bronze zone are the same. Here we update the CRS (see Data Exploration for why), camelCase column names, drop/rename some columns then write to our Bronze zone.\n",
    "\n",
    "As mentioned in the coach notes, parquet does not support ``geometry`` type, so we're saving a geojson as we'll need the geometry later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a741d28f-5784-4c93-b343-28c15f667f18",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "shipwrecksRawFile = '/lakehouse/default/Files/Raw/WAM/Shipwrecks_WAM_002_WA_GDA94_Public.geojson'\n",
    "\n",
    "df_shipwrecks = gp.read_file(shipwrecksRawFile)\n",
    "\n",
    "df_shipwrecks.to_crs('epsg:3857')\n",
    "\n",
    "df_shipwrecks.rename(columns=lambda x: toPascalCase(x), inplace=True)\n",
    "df_shipwrecks.drop(columns={'DateDepth','TimeDepth','MaxDepth','MinDepth','BearingTo','LengthOf','ObjectId','UniqueNum'}, inplace=True)\n",
    "\n",
    "\n",
    "df_shipwrecks.rename(columns={'TypeOfSi': 'Type', 'DateInspe': 'DateInspected', 'Long':'Lon','CountryBu':'CountryBuilt', 'Constructi': 'Construction','PortRegis':'PortRegistered', 'FileNumbe': 'FileNumber','OfficialN':'OfficialNumber','Aac': 'AAC'}, inplace=True)\n",
    "df_shipwrecks.set_geometry(\"Geometry\", inplace=True)\n",
    "df_shipwrecks.to_file(shipwrecksBronzeFile, driver='GeoJSON') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa9460-bca5-4bce-9953-9a7d7f132a9d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Marine Zones Landing From Source\n",
    "\n",
    "The BOM provides marine zone data as a zipped shapefile via FTP. Again, students can either download this from the source, or preferably start with the raw files in resources.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986d6d7-b3ad-40b3-8b2b-a13c50a4ea71",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# WA Marine Forecast Zones IDM000003.zip\n",
    "bomFtpServer = \"ftp://anonymous@ftp.bom.gov.au/\"\n",
    "\n",
    "coastalWatersRawFolder = f\"{rawFilesFolder}BOM\"\n",
    "coastalWatersFile = f\"{bomFtpServer}anon/gen/fwo/IDW11160.xml\"\n",
    "\n",
    "marineZonesRawFolder = f\"{rawFilesFolder}BOM/IDM00003\"\n",
    "marineZonesRawFile = f\"{marineZonesRawFolder}/IDM00003.shp\"\n",
    "marineZonesBronzeFile = f\"{bronzeFilesFolder}marinezones.geojson\"\n",
    "marineZonesZipFile = f\"{bomFtpServer}anon/home/adfd/spatial/IDM00003.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ede7b-af77-4699-9510-d58a66012b8a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Run to download marine zones (or use the resources.zip version instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7defc218-3461-42a2-a5d2-a2cbb9a90800",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(\"Downloading marine zones data....\")\n",
    "marineZonesDownloaded   = downloadFile(marineZonesZipFile, marineZonesRawFolder)\n",
    "unzipFile(marineZonesDownloaded, marineZonesRawFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf7739-9da3-4c6a-a00a-92d5607a6151",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Marine Zones Transforming Raw to Bronze\n",
    "\n",
    "Similar to shipwrecks, here we updae the CRS, camelCase column names, remove/rename cols, and filter to WA coastal waters then writing to Bronze zone as geojson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb3139-51ad-448a-b3b3-7135551422eb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_marineZones = gp.read_file(marineZonesRawFile)\n",
    "\n",
    "df_marineZones.to_crs('epsg:3857')\n",
    "\n",
    "df_marineZones = df_marineZones[df_marineZones.STATE_CODE == \"WA\"]\n",
    "df_marineZones = df_marineZones.where(df_marineZones.notna(), None)\n",
    "df_marineZones.rename(columns=lambda x: toPascalCase(x), inplace=True)\n",
    "df_marineZones.rename(columns={'DistName': 'DistrictName'}, inplace=True) \n",
    "df_marineZones.drop(columns={'DistNo','StateCode','Type', 'Pt1Name','Pt2Name'}, inplace=True)\n",
    "df_marineZones.set_geometry(\"Geometry\", inplace=True)\n",
    "df_marineZones.to_file(marineZonesBronzeFile, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4820f862-599c-4faf-8370-f4d7e91daf26",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Local Water Forecast Landing From Source\n",
    "\n",
    "The BOM provides forecast data as XML via FTP. Again, students can either download this from the source, or preferably start with the raw files in resources.zip. This file is not processed by spark, use a dataflow gen 2, it's easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a27dfb-5bd4-48a1-b500-4b55c2e7eb41",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Download the coastal waters forecast data for later use by a dataflow\n",
    "# or use the version from resources.zip\n",
    "# this file isn't used by this notebook\n",
    "coastalWatersRawFile = downloadFile(coastalWatersFile, coastalWatersRawFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7147d43-8abf-450a-a219-a46d39faebfc",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Challenge 3 - Enriching Shipwrecks with Marine Zone data\n",
    "Using geopandas to spatial join and clean some more before writing to a delta table. For an in-depth explaination see ``Data Exploration``\n",
    "\n",
    "**Note** ensure the geometry column is dropped when writing to Delta!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa072467-3ebf-4906-9ede-1575ef54c365",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n",
    "\n",
    "df_marineZones = gp.read_file(marineZonesBronzeFile)\n",
    "df_shipwrecks = gp.read_file(shipwrecksBronzeFile)\n",
    "\n",
    "df_joined = df_shipwrecks.sjoin_nearest(df_marineZones, how=\"left\", distance_col=\"distance\").query(\"distance < 5000\")\n",
    "\n",
    "#Drop the geometry, it's not supported by parquet\n",
    "df_joined.drop(columns={'index_right', 'geometry', 'distance'}, inplace=True)\n",
    "df_joined = df_joined.where(df_joined.notna(), None)\n",
    "\n",
    "#Save a copy in Silver (not strictly necessary)\n",
    "df_joined.to_json(shipwrecksSilverFile, orient='records', lines=True)\n",
    "\n",
    "#Save to delta\n",
    "saveTable = \"Shipwrecks\"\n",
    "spark.createDataFrame(df_joined).write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").save(f\"Tables/{saveTable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295dbf23-ab03-4099-882f-bec340c81dbb",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Alternatively, if starting from pre-enriched silver/shipwrecks.json in resources.zip \n",
    "(or right click, load data...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a96b1-d2cd-4e63-b086-146c2e41693b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable VOrder write\n",
    "spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n",
    "\n",
    "saveTable = \"Shipwrecks\"\n",
    "df = spark.read.json(shipwrecksSilverFile)\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").save(f\"Tables/{saveTable}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e15aa3-9e8a-438a-9f83-c968825a90dd",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Done!\n",
    "You should now have geojson raw and bronze files, and a silver zone delta table of enriched shipwrecks. \n",
    "\n",
    "Next, **go write a dataflow to import forecast data** (straight to silver perhaps?)\n",
    "\n",
    "----\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "host": {
    "synapse_widget": {
     "state": {},
     "token": "7ab78a9e-4d89-4f31-b4f2-962af741988e"
    }
   },
   "language": "python"
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {},
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
