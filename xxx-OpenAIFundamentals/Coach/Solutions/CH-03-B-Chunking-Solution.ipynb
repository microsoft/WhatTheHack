{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 03-B-Chunking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview \n",
    "\n",
    "In this notebook, you will walk through the concepts of tokens and chunking. In the previous notebook (CH-03-A-Grounding), we were able to provide some additional context to ground the model. Is there a limit to the amount of additional context we can provide the model? Unfortunately, the answer is yes. A limit exists for the number of tokens that are allowed in the input and the output combined based on the model being used.\n",
    "\n",
    "So what are tokens? Tokens are a representation of how the Azure OpenAI models process text. They are words or just chunks of characters. Let's look at the total number of tokens in the response we got back from the first notebook in CH-03. There are many ways to calculate tokens. In this notebook, we will take a look at the tiktoken library to count the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Let's Start Implementation\n",
    "\n",
    "You will need to import the needed modules. The following cells are key setup steps you completed in the previous challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (7.1.2)\n",
      "Collecting click\n",
      "  Using cached click-8.1.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from click) (0.4.6)\n",
      "Installing collected packages: click\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "Successfully installed click-8.1.6\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.30.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.20.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.11)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.59.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (20.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2020.12.5)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from click<9.0.0,>=7.1.1->typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\dthakar\\onedrive\\documents\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade click\n",
    "! python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import PyPDF3\n",
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "import spacy\n",
    "from openai.error import InvalidRequestError\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# from spacy.lang.en import English \n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your environment to access your Azure OpenAI keys. Refer to your Azure OpenAI resource in the Azure Portal to retrieve information regarding your Azure OpenAI endpoint and keys. \n",
    "\n",
    "For security purposes, store your sensitive information in an .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your OpenAI credentials\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert API_KEY, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_BASE\",\"\").strip()\n",
    "assert RESOURCE_ENDPOINT, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in RESOURCE_ENDPOINT.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "model=os.getenv(\"CHAT_MODEL_NAME\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Counting Tokens\n",
    "\n",
    "Tiktoken uses a technique called BPE, or byte pair encoding to convert the given text into tokens. There are different encodings available to help process the words. In this notebook, we will use the cl100k_base.\n",
    "\n",
    "#### Student Challenge #1: \n",
    "\n",
    "Count the number of tokens in the final answer we received in CH-03-A-Grounding by completing the function, count_tokens, below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Challenge #2:\n",
    "\n",
    "Enter in the text from the answer you received in CH-03-A-Grounding. Run the cell below to retrieve the number of tokens using the count_tokens function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 tokens in this sentence: Carlos Alcaraz\n",
      "- Marketa Vondrousova.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Carlos Alcaraz\n",
    "- Marketa Vondrousova.\"\"\"\n",
    "\n",
    "count_tokens(text, \"cl100k_base\")\n",
    "\n",
    "print(\"There are \" + str(count_tokens(text, \"cl100k_base\")) + \" tokens in this sentence: \" + text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now we know how many tokens we are working with. What happens if we want to add in more context than what we already put in the text variable above? If you think about our Wimbleton  scenario, we will need to give the model more context to help it understand everything it needs to know about the tournament. More importantly, everything it needs to know to help answer your questions when writing the report! Let's say we want to provide more context to the model with a PDF document. Can we try to get a summary of the PDF document to help us with our paper?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Challenge #3: \n",
    "\n",
    "In the cell below, insert the path of the PDF document given to you in the Resources.zip file. Run the three cells to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = open(r'C:\\Users\\dthakar\\Documents\\GitHub\\WhatTheHack\\xxx-OpenAIFundamentals\\Student\\Resources\\data\\CH3-data.pdf', 'rb')\n",
    "doc_helper = PyPDF3.PdfFileReader(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaltext = ''\n",
    "totalpages = doc_helper.getNumPages()\n",
    "for eachpage in range(totalpages):\n",
    "   p = doc_helper.getPage(eachpage)\n",
    "   indpagetext = p.extractText()\n",
    "   finaltext += indpagetext\n",
    "\n",
    "clean_text = finaltext.replace(\"  \", \" \").replace(\"\\n\", \"; \").replace(';',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"What is the answer to the following question regarding the PDF document?\\n\\n{finaltext}\\n\\n\" \n",
    "q = \"Can you give me a summary of the document?\"\n",
    "\n",
    "try:\n",
    "    final_prompt = prompt + q\n",
    "    response = openai.Completion.create(engine=model, prompt=final_prompt, max_tokens=50)\n",
    "    answer = response.choices[0].text.strip()\n",
    "    print(f\"{q}\\n{answer}\\n\")\n",
    "\n",
    "except InvalidRequestError as e:\n",
    "    print(e.error)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see above, you will get an error message after running the above snippet of code. The model reaches its maximum context length. For GPT-3 models, the token limit is 4097 tokens. How do we fix this issue by giving it all of the needed context, but not running into the token limit issue?\n",
    "\n",
    "To solve this problem, we can take a look at a concept called Chunking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunking\n",
    "\n",
    "Chunking helps limit the amount of information we pass into the model. The information that we will pass through are the most relevant chunks from the overall data. There are many considerations that come into play when chunking. For example, you need to figure out the best chunk size. If the chunks are too small, you may lose important context. If the chunks are too big, it may contain unnecessary information. \n",
    "\n",
    "Below are some common chunking techniques.\n",
    "\n",
    "1. Chunking with smaller chunks \n",
    "2. Chunking by splitting sentences  \n",
    "3. Chunking with sentence overlap \n",
    "4. Chunking recursively \n",
    "\n",
    "Let us take a look at these techniques in action."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Chunking with smaller chunks \n",
    "\n",
    "#### Student Challenge #4: Add code in the cell below. Use the split() function to chunk the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"The sun was setting over the horizon, casting a warm glow over the landscape. Birds chirped in the trees, and a gentle breeze rustled the leaves. In the distance, a herd of deer grazed in a meadow. The air was filled with the sweet scent of blooming flowers. It was a peaceful and serene scene, perfect for a quiet evening stroll.\"\n",
    "\n",
    "chunks = text.split()\n",
    "\n",
    "for chunk in chunks: \n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you observe about the chunks returned? If each chunk stood by itself, would you be able to understand the semantic meaning?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Chunking by splitting sentences\n",
    "\n",
    "#### Student Challenge #5:Add code in the cell below. Use the spacy library and specifically sents function to chunk the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Today was a fun day. I had lots of ice cream. I also met my best friend Sally and we played together at the new playground.\"\n",
    "\n",
    "for sentence in nlp(text).sents:\n",
    "    print(sentence.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results better than the method in 4.1? The spaCy library helps toto chunk the text into individual sentences. This can be useful when you are trying to do text summarization. You can rank the individual sentences and use the top results in the summary.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Chunking with sentence overlap \n",
    "\n",
    "#### Student Challenge #6: Run the code below to see another example of chunking. As you will see, the semantic meaning is kept. In other words, the context is preserved between the sentences. This is especially important when you are searching data for relevant results or when you are summarizing a piece of text. It is important to capture the relationships between the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The sun was setting over the horizon, casting a warm glow over the landscape. Birds chirped in the trees, and a gentle breeze rustled the leaves. In the distance, a herd of deer grazed in a meadow. The air was filled with the sweet scent of blooming flowers. It was a peaceful and serene scene, perfect for a quiet evening stroll.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "overlap = 1\n",
    "chunks =[]\n",
    "\n",
    "for i in range(len(sentences) - overlap):\n",
    "    chunk = sentences[i : i + overlap + 1]\n",
    "    chunks.append(chunk)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print([sent.text for sent in chunk])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4: Chunking recursively using LangChain\n",
    "\n",
    "#### Student Challenge #7: Add in the required parameters for the RecursiveCharacterSplitter in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 30 \n",
    ")\n",
    "docs = split_text.create_documents([clean_text])\n",
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we did some chunking using langchain, a popular framework for creating applications using large language models. In the previous methods you saw various examples of chunking. Langchain can help make the chunking process easier with some of its methods. These methods include fixed size chunks as well as recursive chunking, which we saw just now.\n",
    "\n",
    "For example, there is CharacterTextSplitter which will split the given text into a fixed size chunk of a given size and a given overlap of characters. \n",
    "\n",
    "RecursiveCharacterTextSplitter divides the text into smaller chunks in an iterative manner. Again, you can provide the chunk size and chunk overlap count. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is an important technique for many reasons. It helps bypass the token limit when working with lots of data and also optimizes the response we get back from the model. Finding the right chunking technique and chunk size is crucial to receiving relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success Criteria\n",
    "\n",
    "To complete this challenge successfully:\n",
    "\n",
    "* Show an understanding of tokens and how to calculate them.\n",
    "* Show an understanding of chunking by experimenting with different techniques.\n",
    "* Be able to understand the importance of finding the right chuning solution based on if the semantic meaning is getting captured or not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
